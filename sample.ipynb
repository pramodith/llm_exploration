{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import chroma\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('sotu.txt', encoding='utf-8').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import re\n",
    "\n",
    "system_message = \"What questions does this document answer?, Each question should start from a new line and the questions should be specific and not too vague. \\n Document: \"\n",
    "text = \"{document}\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message),\n",
    "    (\"human\", text)\n",
    "])\n",
    "messages = [HumanMessage(content=text)]\n",
    "question_docs = {}\n",
    "for ind, document in enumerate(documents):\n",
    "    if ind % 10 == 0:\n",
    "        print(ind)\n",
    "    message = chat_prompt.format_messages(document=document)\n",
    "    output = llm(message)\n",
    "    for question in output.content.split(\"\\n\"):\n",
    "        # remove special characters using regex\n",
    "        question = re.sub(r'[^a-zA-Z0-9\\s]', '', question)     \n",
    "        # remove leading numbers\n",
    "        question = re.sub(r'^\\d+\\s+', '', question)   \n",
    "        question_docs[question] = document.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the questions and the corresponding document to a json file\n",
    "import json\n",
    "with open('questions.json', 'w') as fp:\n",
    "    json.dump(question_docs, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class VectorStore with abstract methods add_texts, from_texts, similarity_search",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pramo\\ProgrammingProjects\\RandomLLM\\sample.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pramo/ProgrammingProjects/RandomLLM/sample.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m llm \u001b[39m=\u001b[39m langchain\u001b[39m.\u001b[39mOpenAI()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pramo/ProgrammingProjects/RandomLLM/sample.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Define the vector store\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pramo/ProgrammingProjects/RandomLLM/sample.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m vector_store \u001b[39m=\u001b[39m chroma\u001b[39m.\u001b[39;49mVectorStore()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pramo/ProgrammingProjects/RandomLLM/sample.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Define a function to retrieve the top-k relevant documents\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pramo/ProgrammingProjects/RandomLLM/sample.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mretrieve_documents\u001b[39m(query, k\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pramo/ProgrammingProjects/RandomLLM/sample.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Compute the similarity between the query and all embedding questions in the vector store\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can't instantiate abstract class VectorStore with abstract methods add_texts, from_texts, similarity_search"
     ]
    }
   ],
   "source": [
    "# Define the LLM model\n",
    "# Define the vector store\n",
    "vector_store = chroma.VectorStore()\n",
    "\n",
    "# Define a function to retrieve the top-k relevant documents\n",
    "def retrieve_documents(query, k=5):\n",
    "    # Compute the similarity between the query and all embedding questions in the vector store\n",
    "    similarities = vector_store.similarity(query)\n",
    "    \n",
    "    # Retrieve the document chunks that correspond to the top-k relevant questions\n",
    "    top_k_questions = similarities.argsort()[-k:][::-1]\n",
    "    top_k_documents = [vector_store.get_document(question) for question in top_k_questions]\n",
    "    \n",
    "    return top_k_documents\n",
    "\n",
    "# Define a function to add a document chunk to the vector store\n",
    "def add_document(document_chunk):\n",
    "    # Ask the LLM \"What questions can this document answer?\"\n",
    "    questions = llm.generate(document_chunk, num_return_sequences=5)\n",
    "    \n",
    "    # Embed the question and document chunk into the vector store\n",
    "    for question in questions:\n",
    "        embedding = langchain.embed(question + document_chunk)\n",
    "        vector_store.add_document(embedding, document_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PromptRiddler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b15fad5b33d375c0d54baf452e86479af51214702d63df155e0507416a07d601"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
