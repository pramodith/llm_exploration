{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pramodith/llm_exploration/blob/colab/dynamic_prompt_token_dropping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAocVSrQ1dj4",
        "outputId": "fdd1194e-e16f-45cc-b81b-93eea7324139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.335)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.64)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install langchain\n",
        "!pip install datasets\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ts1-kCDF1YMe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from typing import List\n",
        "import torch\n",
        "from pprint import pprint\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from langchain.callbacks import get_openai_callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1lNP9lMA1YMf"
      },
      "outputs": [],
      "source": [
        "model_name = \"facebook/opt-125m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "RxHHXF-J1YMg"
      },
      "outputs": [],
      "source": [
        "def to_tokens_and_probs(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, input_texts: List[str]):\n",
        "    \"\"\"\n",
        "    This function takes a list of input texts and returns a list of tuples (token, prob) for each token in the input text.\n",
        "    Reference: https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075/17\n",
        "\n",
        "    Args:\n",
        "        model (AutoModelForCausalLM): _description_\n",
        "        tokenizer (AutoTokenizer): _description_\n",
        "        input_texts (List[str]): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer(input_texts, padding=True, return_tensors=\"pt\").input_ids\n",
        "    outputs = model(input_ids)\n",
        "    probs = torch.softmax(outputs.logits, dim=-1).detach()\n",
        "\n",
        "    # collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1\n",
        "    probs = probs[:, :-1, :]\n",
        "    input_ids = input_ids[:, 1:]\n",
        "    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)\n",
        "\n",
        "    batch = []\n",
        "    for input_sentence, input_probs in zip(input_ids, gen_probs):\n",
        "        text_sequence = []\n",
        "        for token_pos, (token, p) in enumerate(zip(input_sentence, input_probs)):\n",
        "            if token not in tokenizer.all_special_ids:\n",
        "                text_sequence.append((tokenizer.decode(token), p.item(), token_pos, token.item()))\n",
        "        batch.append(text_sequence)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cIlDK4Vg1YMg"
      },
      "outputs": [],
      "source": [
        "sample_prompt = [\"The capital of France is Paris.\"]\n",
        "token_probs = to_tokens_and_probs(model, tokenizer, sample_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIIru2DF1YMg",
        "outputId": "49a0b22f-967d-45fb-9ca5-7c4abfd5b02e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 0.08746562898159027, 0, 133),\n",
              "  (' capital', 8.743484795559198e-05, 1, 812),\n",
              "  (' of', 0.14376769959926605, 2, 9),\n",
              "  (' France', 0.005475882440805435, 3, 1470),\n",
              "  (' is', 0.24484379589557648, 4, 16),\n",
              "  (' Paris', 0.0067796302028000355, 5, 2201),\n",
              "  ('.', 0.33608853816986084, 6, 4)]]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "token_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBYIcd8I2bwk"
      },
      "source": [
        "Set your key to openai's API's using colab's secrets features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "gk6ILZrQ13MY"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "GPT_35_TURBO_COST_PER_INP_TOKEN = 0.0010/1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "r-xNTPY32sdX"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6A_kecpHJNQN"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "class ReadingComprehensionPrompt(BaseModel):\n",
        "    system_message: str = SystemMessagePromptTemplate.from_template(f\"You are a very smart student in a reading comprehension class.\"\n",
        "        \"Your teacher is giving you a reading comprehension test. You are given a passage and a question.\"\n",
        "        \"You must answer the question based on the passage.\")\n",
        "    human_message: str = HumanMessagePromptTemplate.from_template(\"Passage: {passage}\\nQuestion: {question}\\nAnswer: \")\n",
        "\n",
        "rc_prompt = ReadingComprehensionPrompt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XEt36w2uJNQO"
      },
      "outputs": [],
      "source": [
        "def get_llm_response(prompt: BaseModel, chat: ChatOpenAI, passage: str, question: str):\n",
        "    chat_message = ChatPromptTemplate.from_messages([prompt.system_message, prompt.human_message])\n",
        "    completed_prompt = chat_message.format_prompt(passage=passage, question=question).to_messages()\n",
        "    with get_openai_callback() as cb:\n",
        "      response = chat(completed_prompt)\n",
        "    return response, cb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "jCC8dqeiJNQO",
        "outputId": "83e76156-9a2c-492b-d59e-9b26a31635bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(AIMessage(content='The passage does not provide any information about the capital of Australia.'),\n",
              " Tokens Used: 86\n",
              " \tPrompt Tokens: 73\n",
              " \tCompletion Tokens: 13\n",
              " Successful Requests: 1\n",
              " Total Cost (USD): $0.0001355)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "get_llm_response(rc_prompt, chat, \"The capital of France is Paris.\", \"What is the capital of Australia?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "oBefzhcxJNQO"
      },
      "outputs": [],
      "source": [
        "def sample_squad_dataset(num_samples: int = 100):\n",
        "    dataset = load_dataset(\"squad\", split=\"validation\")\n",
        "    return dataset.shuffle().select(range(num_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "6wgsf1xuJNQP"
      },
      "outputs": [],
      "source": [
        "squad_dataset = sample_squad_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squad_dataset[0]"
      ],
      "metadata": {
        "id": "fVOQP_awM_mi",
        "outputId": "8fcb0f05-1953-4eac-fa89-76185e4ea030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56e76abf37bdd419002c3f75',\n",
              " 'title': 'Teacher',\n",
              " 'context': \"Teachers face several occupational hazards in their line of work, including occupational stress, which can negatively impact teachers' mental and physical health, productivity, and students' performance. Stress can be caused by organizational change, relationships with students, fellow teachers, and administrative personnel, working environment, expectations to substitute, long hours with a heavy workload, and inspections. Teachers are also at high risk for occupational burnout.\",\n",
              " 'question': \"What can hurt a teacher's mental and physical health?\",\n",
              " 'answers': {'text': ['occupational stress',\n",
              "   'occupational stress',\n",
              "   'occupational stress'],\n",
              "  'answer_start': [76, 76, 76]}}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "IHrL9iAEJNQP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def get_token_dropped_text(doc: str, tokenizer:AutoTokenizer, dropout_percent=0.1) -> str:\n",
        "    token_probs = to_tokens_and_probs(model, tokenizer, [doc])\n",
        "    tokens = [token[-1] for token in token_probs[0]]\n",
        "    num_tokens_to_drop = int(len(tokens)*dropout_percent)\n",
        "    top_10_percent_tokens = sorted(token_probs[0], key=lambda x: x[1], reverse=True)[:len(tokens) // 10]\n",
        "    dropped_tokens = [t[0] for t in top_10_percent_tokens]\n",
        "    tokens_after_deletion = np.delete(tokens, [token[2] for token in top_10_percent_tokens])\n",
        "    # Remove the top 10% of tokens\n",
        "    dropped_token_text = tokenizer.decode(tokens_after_deletion)\n",
        "    return dropped_token_text, dropped_tokens\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpATjNkSJNQP",
        "outputId": "20c244fe-01ff-4a85-ff83-c86e3c96c3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual answer is {'text': ['occupational stress', 'occupational stress', 'occupational stress'], 'answer_start': [76, 76, 76]}\n",
            "Answer for dropped text is content=\"Occupational stress can hurt a teacher's mental and physical health.\", cost is 0.00014099999999999998\n",
            "Answer for original text is content=\"Occupational stress can hurt a teacher's mental and physical health.\", cost is 0.00015\n",
            "Actual answer is {'text': ['Baden-Württemberg', 'Baden-Württemberg', 'Baden-Württemberg', 'Baden-Württemberg'], 'answer_start': [323, 323, 323, 323]}\n",
            "Answer for dropped text is content='Lake Constance separates the German state Bavaria from the Austrian state Vorarlberg.', cost is 0.00036399999999999996\n",
            "Answer for original text is content='Lake Constance separates the German state Bavaria from the German state of Baden-Württemberg.', cost is 0.00038599999999999995\n",
            "Actual answer is {'text': ['bans', 'bans on foreign popular culture, control of the internet and unauthorised satellite dishes', 'bans on foreign popular culture, control of the internet and unauthorised satellite dishes', 'bans'], 'answer_start': [765, 765, 765, 765]}\n",
            "Answer for dropped text is content='Regimes fight against cultural imperialism by implementing bans on foreign popular culture, controlling the internet, and prohibiting unauthorized satellite dishes.', cost is 0.000614\n",
            "Answer for original text is content='Regimes fight against cultural imperialism by implementing bans on foreign popular culture, controlling the internet, and prohibiting unauthorized satellite dishes.', cost is 0.000655\n",
            "Actual answer is {'text': ['Charles and Ray Eames', 'Charles and Ray Eames', 'Charles and Ray Eames'], 'answer_start': [815, 815, 815]}\n",
            "Answer for dropped text is content='The husband and wife modern furniture design team represented in the V&A furniture collection is Charles and Ray Eames.', cost is 0.000882\n",
            "Answer for original text is content='The husband and wife modern furniture design team represented in the V&A furniture collection is Charles and Ray Eames.', cost is 0.000936\n",
            "Actual answer is {'text': ['inequality-associated effects', 'inequality-associated effects', 'inequality'], 'answer_start': [622, 622, 622]}\n",
            "Answer for dropped text is content='Policies which try to control unemployment support economic growth because they reduce inequality-associated effects.', cost is 0.001063\n",
            "Answer for original text is content='Policies which try to control unemployment support economic growth because they reduce inequality.', cost is 0.001124\n",
            "Actual answer is {'text': ['Isaac Newton', 'Isaac Newton', 'Isaac Newton', 'Isaac Newton'], 'answer_start': [125, 125, 125, 125]}\n",
            "Answer for dropped text is content='Isaac Newton formed the universal theory of gravitation.', cost is 0.0013989999999999999\n",
            "Answer for original text is content='Isaac Newton formed the universal theory of gravitation.', cost is 0.001488\n",
            "Actual answer is {'text': ['Finsteraarhorn', 'summit of Finsteraarhorn', 'Finsteraarhorn'], 'answer_start': [412, 402, 412]}\n",
            "Answer for dropped text is content='The highest point of the Rhine basin is called the Finsteraarhorn.', cost is 0.0016389999999999998\n",
            "Answer for original text is content='The highest point of the Rhine basin is called Finsteraarhorn.', cost is 0.001743\n",
            "Actual answer is {'text': [\"the d'Hondt method\", \"d'Hondt\", \"the d'Hondt method\"], 'answer_start': [155, 159, 155]}\n",
            "Answer for dropped text is content=\"The d'Hond method is used for tallying votes in the second vote of the ballot.\", cost is 0.0018199999999999998\n",
            "Answer for original text is content=\"The d'Hondt method is used for tallying votes in the second vote of the ballot.\", cost is 0.001936\n",
            "Actual answer is {'text': ['Iroquois rule, and were limited by them in authority to make agreements', 'Iroquois', 'Iroquois', 'Iroquois', 'Iroquois'], 'answer_start': [565, 565, 565, 565, 565]}\n",
            "Answer for dropped text is content='Some native tribes lived under the rule of the Iroquois Confederation.', cost is 0.0020199999999999997\n",
            "Answer for original text is content='Some native tribes lived under the rule of the Iroquois Confederation.', cost is 0.002147\n"
          ]
        }
      ],
      "source": [
        "total_dropped_cost = 0\n",
        "total_cost = 0\n",
        "dropped_tokens = []\n",
        "for i in range(10):\n",
        "  dropped_context, dropped_context_text = get_token_dropped_text(squad_dataset[i][\"context\"], tokenizer)\n",
        "  dropped_question, dropped_question_text = get_token_dropped_text(squad_dataset[i][\"question\"], tokenizer)\n",
        "  answer, cb = get_llm_response(rc_prompt, chat, passage = squad_dataset[i][\"context\"], question = squad_dataset[i][\"question\"])\n",
        "  dropped_answer, cb_d = get_llm_response(rc_prompt, chat, passage=dropped_context, question=dropped_question)\n",
        "  dropped_cost = cb_d.prompt_tokens*GPT_35_TURBO_COST_PER_INP_TOKEN\n",
        "  cost = cb.prompt_tokens*GPT_35_TURBO_COST_PER_INP_TOKEN\n",
        "  total_dropped_cost += dropped_cost\n",
        "  total_cost += cost\n",
        "  print(f\"Actual answer is {squad_dataset[i]['answers']}\")\n",
        "  print(f\"Answer for dropped text is {dropped_answer}, cost is {total_dropped_cost}\")\n",
        "  print(f\"Answer for original text is {answer}, cost is {total_cost}\")\n",
        "  squad_dataset[i][\"predicted_answer\"] = answer\n",
        "  squad_dataset[i][\"predicted_answer_for_dropped\"] = dropped_answer\n",
        "  dropped_tokens.extend(dropped_context_text)\n",
        "  dropped_tokens.extend(dropped_question_text)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A0KJvVmxM--R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "PromptRiddler",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}