{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pramodith/llm_exploration/blob/bert_sparse_attention_training/bert_sparse_attention_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWhWL9msORXO"
      },
      "source": [
        "# Exploring the affects of custom Sparse Attention\n",
        "In our prior notebook, we found that both encoder-only and decoder-only models offload a significant portion of their attention scores to **sink tokens**. We identified that these sink tokens tend to be either special tokens like **[CLS], [SEP]** or tokens corresponding to **punctuations**. The consistence display of this phenomenon across model architectures and inputs makes one question the relevance of dense self-attention.\n",
        "\n",
        "In this notebook we'll explore the performance of BERT by creating custom attention masks, which will be sparse in nature. We'll create a unique mask per each token, where all tokens attend to special tokens and the k tokens in their neighborhood. When visualized the tokens along a diagonal of size 2*k+1 and the first anad last tokens (in the case of BERT) being attended to. We'll also explore the effects of allowing dense attention in some layers and sparse attention in the rest.\n",
        "\n",
        "We'll assess the downstream performance of the models that leverage this type of custom attention mask on some commonly used datasets for benchmarking like [TBD]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBBUafqnJPeP",
        "outputId": "b424264a-0af2-47a9-df24-5411aa81c886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.10/dist-packages (7.4.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers\n",
        "%pip install datasets\n",
        "%pip install accelerate -U\n",
        "%pip install scikit-learn\n",
        "%pip install overrides\n",
        "%pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "53fY5yW9JNUS"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "from functools import partial\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset, load_metric, load_from_disk, Dataset, Metric\n",
        "from transformers import TrainingArguments, Trainer,logging\n",
        "from evaluate import load\n",
        "import torch\n",
        "\n",
        "from transformers import BertModel, BertForSequenceClassification, BertForMaskedLM\n",
        "from transformers.data.data_collator import _torch_collate_batch\n",
        "from transformers.models.bert.modeling_bert import BertEncoder, logger\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n",
        "from transformers.modeling_utils import ModuleUtilsMixin, warnings\n",
        "from typing import Any, Dict, Optional, Tuple, Union, List, Mapping\n",
        "from overrides import overrides\n",
        "from torch import Tensor\n",
        "from torch.nn.functional import pad\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H_MITsQuJNUU"
      },
      "outputs": [],
      "source": [
        "# Defining the model, tokenizer and dataset\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "sample_text = \"Every night I lie in bed.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqOC7C2dORXR"
      },
      "source": [
        "### Get Dataset Splits and Metrics to Evaluate Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ihbuJxkbJNUV"
      },
      "outputs": [],
      "source": [
        "def get_dataset(dataset_name: str, subset_name: Optional[str]) -> Tuple[Dataset, Dataset, Dataset, Metric]:\n",
        "    \"\"\"\n",
        "    This function loads the dataset and metric for the given task name.\n",
        "    It also splits the dataset into train, dev and test sets.\n",
        "\n",
        "    Args:\n",
        "        glue_task_name (str): The name of the task to be loaded.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dataset, Dataset, Dataset, int]: The train, dev and test datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the dataset and metric\n",
        "    dataset = load_dataset(dataset_name, subset_name)\n",
        "    # Split the dataset\n",
        "    train_dataset = dataset[\"train\"]\n",
        "    if \"validation\" not in dataset or \"test\" not in dataset:\n",
        "        split = train_dataset.train_test_split(test_size=0.1)\n",
        "        if \"validation\" not in dataset:\n",
        "            dev_dataset = split[\"test\"]\n",
        "            test_dataset = dataset[\"test\"]\n",
        "        elif \"test\" not in dataset:\n",
        "            dev_dataset = dataset[\"validation\"]\n",
        "            test_dataset = split[\"test\"]\n",
        "    else:\n",
        "        print(f\"{dataset_name} has already been split into train, dev and test sets.\")\n",
        "        dev_dataset = dataset['validation']\n",
        "        test_dataset = dataset['test']\n",
        "\n",
        "    # Print a description of the dataset\n",
        "    print(\"Dataset Description: \", train_dataset.description)\n",
        "\n",
        "    # Truncate the dataset if it is too large\n",
        "    if train_dataset.num_rows > 30000:\n",
        "        train_dataset = train_dataset.select(range(30000))\n",
        "\n",
        "    # Print the label space\n",
        "    print(\"Label Space: \", train_dataset.features[\"label\"].names)\n",
        "    num_labels = len(train_dataset.features[\"label\"].names)\n",
        "    return train_dataset, dev_dataset, test_dataset, num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT26JWyDJNUW",
        "outputId": "9cf3f273-0a9a-4259-d4e8-f94747f33dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dair-ai/emotion has already been split into train, dev and test sets.\n",
            "Dataset Description:  Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper.\n",
            "\n",
            "Label Space:  ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
            "Dataset Description:  AG is a collection of more than 1 million news articles. News articles have been\n",
            "gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of\n",
            "activity. ComeToMyHead is an academic news search engine which has been running\n",
            "since July, 2004. The dataset is provided by the academic comunity for research\n",
            "purposes in data mining (clustering, classification, etc), information retrieval\n",
            "(ranking, search, etc), xml, data compression, data streaming, and any other\n",
            "non-commercial activity. For more information, please refer to the link\n",
            "http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
            "\n",
            "The AG's news topic classification dataset is constructed by Xiang Zhang\n",
            "(xiang.zhang@nyu.edu) from the dataset above. It is used as a text\n",
            "classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann\n",
            "LeCun. Character-level Convolutional Networks for Text Classification. Advances\n",
            "in Neural Information Processing Systems 28 (NIPS 2015).\n",
            "\n",
            "Label Space:  ['World', 'Sports', 'Business', 'Sci/Tech']\n",
            "tweet_eval has already been split into train, dev and test sets.\n",
            "Dataset Description:  TweetEval consists of seven heterogenous tasks in Twitter, all framed as multi-class tweet classification. All tasks have been unified into the same benchmark, with each dataset presented in the same format and with fixed training, validation and test splits.\n",
            "\n",
            "Label Space:  ['non-offensive', 'offensive']\n"
          ]
        }
      ],
      "source": [
        "metrics = [load('accuracy'), load(\"f1\"), load(\"precision\"), load(\"recall\")]\n",
        "dataset_names = [(\"dair-ai/emotion\",\"\"),(\"ag_news\",\"\"),(\"tweet_eval\",\"offensive\")]\n",
        "train_datasets, dev_datasets, test_datasets, num_labels, train_datasets_size = [], [], [], [], []\n",
        "for dataset_name, subset_name in dataset_names:\n",
        "    train_dataset, dev_dataset, test_dataset, num_label = get_dataset(dataset_name, subset_name)\n",
        "    train_datasets.append(train_dataset)\n",
        "    dev_datasets.append(dev_dataset)\n",
        "    test_datasets.append(test_dataset)\n",
        "    num_labels.append(num_label)\n",
        "    train_datasets_size.append(train_dataset.num_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D8BS7cfJ9UNx"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred: Tuple) -> Dict:\n",
        "    \"\"\"\n",
        "    This function computes the metrics for the given task and is used by the Trainer class for evaluation.\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=1)\n",
        "    results = {}\n",
        "    for metric in metrics:\n",
        "      if metric.name != \"accuracy\":\n",
        "        results.update(metric.compute(predictions=predictions, references=labels, average=\"macro\"))\n",
        "      else:\n",
        "        results.update(metric.compute(predictions=predictions, references=labels))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDU7zJb4ORXS"
      },
      "source": [
        "## Create Custom Bert Model to support 4-D attention masks\n",
        "HF doesn't support custom layerwise attention masks. By default it assumes that all layers use the same attention mask. However,we want to be able to experiment with using sparse attention in all the layers but also a hybrid of dense and sparse self attention split across layers.\n",
        "\n",
        "In order to support this functionality we'll need to override some functions and create Custom Bert Models.\n",
        "\n",
        "First we'll override the `forward` function of `BertEncoder` to accept 5-d attention masks. The 5-d mask corresponds to (Layer, Batch Size, Attention Head, Sequence length (from token), Sequence length (to token)). We assume that all attention heads share the same attention mask in this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jk-pBQF5UBlI"
      },
      "outputs": [],
      "source": [
        "class CustomBertEncoder(BertEncoder):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    @overrides\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        output_hidden_states: Optional[bool] = False,\n",
        "        return_dict: Optional[bool] = True,\n",
        "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        if self.gradient_checkpointing and self.training:\n",
        "            if use_cache:\n",
        "                logger.warning_once(\n",
        "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                )\n",
        "                use_cache = False\n",
        "\n",
        "        attention_mask_is_layerwise = False\n",
        "\n",
        "        # Code added here\n",
        "        if attention_mask.dim() == 5:\n",
        "            attention_mask_is_layerwise = True\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            # Code added here\n",
        "            if attention_mask_is_layerwise:\n",
        "               attention_mask_to_use = attention_mask[i]\n",
        "            else:\n",
        "                attention_mask_to_use = attention_mask\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                layer_outputs = self._gradient_checkpointing_func(\n",
        "                    layer_module.__call__,\n",
        "                    hidden_states,\n",
        "                    attention_mask_to_use,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask_to_use,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DwTI4fUORXS"
      },
      "source": [
        "Next we'll override `get_extended_attention_mask` of the `BertModel` class to create a new dimension expanding 4-d attention masks to 5-d attention masks. The expanded dimension corresponds to attention heads as mentioned above since we assume that all attention heads use the same mask we just need to add a new dimension to the mask tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5npfO9CfORXS"
      },
      "outputs": [],
      "source": [
        "class CustomBertModel(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.encoder = CustomBertEncoder(config)\n",
        "\n",
        "    @overrides\n",
        "    def get_extended_attention_mask(\n",
        "        self, attention_mask: Tensor, input_shape: Tuple[int], device: torch.device = None, dtype: torch.float = None\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
        "\n",
        "        Arguments:\n",
        "            attention_mask (`torch.Tensor`):\n",
        "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
        "            input_shape (`Tuple[int]`):\n",
        "                The shape of the input to the model.\n",
        "\n",
        "        Returns:\n",
        "            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n",
        "        \"\"\"\n",
        "        if dtype is None:\n",
        "            dtype = self.dtype\n",
        "\n",
        "        if not (attention_mask.dim() == 2 and self.config.is_decoder):\n",
        "            # show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`\n",
        "            if device is not None:\n",
        "                warnings.warn(\n",
        "                    \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n",
        "                )\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        # Code added here\n",
        "        if attention_mask.dim() == 4:\n",
        "            extended_attention_mask = attention_mask[:, :, None, :, :]\n",
        "        elif attention_mask.dim() == 3:\n",
        "            extended_attention_mask = attention_mask[:, None, :, :]\n",
        "        elif attention_mask.dim() == 2:\n",
        "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
        "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
        "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "            if self.config.is_decoder:\n",
        "                extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(\n",
        "                    input_shape, attention_mask, device\n",
        "                )\n",
        "            else:\n",
        "                extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n",
        "            )\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and the dtype's smallest value for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n",
        "        return extended_attention_mask\n",
        "\n",
        "    \"\"\"\n",
        "    @overrides\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
        "        r\n",
        "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc-217gbORXT"
      },
      "source": [
        "#### Create a custom bert model for sequence classification that uses the custom bert encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U37R2yvDORXT"
      },
      "outputs": [],
      "source": [
        "class CustomBertForSequenceClassification(BertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = CustomBertModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o6mABgKyVD-3"
      },
      "outputs": [],
      "source": [
        "def torch_mask_tokens(tokenizer: AutoTokenizer, mlm_probability: float, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
        "    \"\"\"\n",
        "    Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
        "    \"\"\"\n",
        "    import torch\n",
        "\n",
        "    labels = inputs.clone()\n",
        "    # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
        "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
        "    if special_tokens_mask is None:\n",
        "        special_tokens_mask = [\n",
        "            tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "        ]\n",
        "        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "    else:\n",
        "        special_tokens_mask = special_tokens_mask.bool()\n",
        "\n",
        "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9q8vDZFORXT"
      },
      "source": [
        "The `custom_tokenize` function tokenizes a given text and creates a custom attention mask for the BERT model.\n",
        "\n",
        "The function takes four arguments: a tokenizer, a text string, a distance integer (default is 2), and a batch_mode boolean (default is False).\n",
        "\n",
        "The function tokenizes the text and creates an attention mask with ones on the main diagonal. It then updates the attention mask based on the specified neighborhood distance. The function ensures that the first and last tokens (usually the CLS and SEP tokens in BERT) always attend to all other tokens and are attended to by all other tokens.\n",
        "\n",
        "If `batch_mode` is `True`, the function adds an extra dimension to the attention mask, input ids, and token type ids to accommodate batch processing.\n",
        "\n",
        "The function returns a dictionary containing the tokenized input ids, token type ids, and the custom attention mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qPhmj3Zy9UNy"
      },
      "outputs": [],
      "source": [
        "def custom_tokenize(tokenizer: AutoTokenizer, text: str, distance:int=2, batch_mode=False) -> Dict:\n",
        "    \"\"\"\n",
        "    This function tokenizes the given text and returns the tokenized inputs along with the attention mask.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (AutoTokenizer): The tokenizer to be used for tokenization.\n",
        "        text (str): The text to be tokenized.\n",
        "        distance (int, optional): The number of neigbhoring tokens that should be attended to.\n",
        "        batch_mode (bool, optional): If we need to add an extra dimension for batch processing.\n",
        "\n",
        "    Returns:\n",
        "        Dict: The tokenized inputs along with the attention mask.\n",
        "    \"\"\"\n",
        "    # Tokenize the texts\n",
        "    result = tokenizer(text, truncation=True, padding=False)\n",
        "    # Create attention mask with ones on the main diagonal\n",
        "    attention_mask = torch.eye(len(result[\"input_ids\"]), dtype=torch.long)\n",
        "    # Update attention mask for the specified neighborhood distance\n",
        "    attention_mask[abs(torch.arange(len(attention_mask))[:, None] - torch.arange(len(attention_mask))) <= distance] = 1\n",
        "\n",
        "    # Set the first row to 1 corresponding to the CLS token\n",
        "    attention_mask[0, :] = 1\n",
        "    # Always attend to CLS\n",
        "    attention_mask[:, 0] = 1\n",
        "    # Set the last row to 1 corresponding to the SEP token\n",
        "    attention_mask[-1, :] = 1\n",
        "    # Always attend to SEP\n",
        "    attention_mask[:, -1] = 1\n",
        "    # Add the attention mask to the result\n",
        "\n",
        "    if batch_mode:\n",
        "      result[\"attention_mask\"] = torch.LongTensor(attention_mask.unsqueeze(0))\n",
        "      result[\"input_ids\"] = torch.LongTensor(result[\"input_ids\"]).unsqueeze(0)\n",
        "      result[\"token_type_ids\"] = torch.LongTensor(result[\"token_type_ids\"]).unsqueeze(0)\n",
        "    else:\n",
        "      result[\"attention_mask\"] = torch.LongTensor(attention_mask)\n",
        "      result[\"input_ids\"] = torch.LongTensor(result[\"input_ids\"])\n",
        "      result[\"token_type_ids\"] = torch.LongTensor(result[\"token_type_ids\"])\n",
        "    # Map the labels to the tokenized inputs\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whRhmsDrORXU"
      },
      "source": [
        "Let's make sure our implementation is right by visualizing a heatmap of our attention scores. We should expect to see a sliding window of attention along the diagonal while the **[CLS]** and **[SEP]** tokens are allowed to attend to all tokens and all tokens are allowed to attend to the **[CLS]** and **[SEP]** tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOdvYb8DdHZN",
        "outputId": "aa67be68-06f5-41dd-88f3-c1e70a4853c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "custom_model = CustomBertForSequenceClassification.from_pretrained(model_name)\n",
        "inputs = custom_tokenize(tokenizer, sample_text, distance=1, batch_mode=True)\n",
        "output = custom_model(**inputs, output_attentions=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "id": "VDu0KYhMORXU",
        "outputId": "f4e07765-dd44-4712-ecfb-93d870814d36"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the attention scores for Layer 0, Batch 0, Head 0\n",
        "attention_scores = output.attentions[0][0,0].detach().numpy()\n",
        "\n",
        "# Get the tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(attention_scores, cmap='hot', interpolation='nearest')\n",
        "plt.colorbar(label='Attention Scores')\n",
        "plt.title('Attention Scores Heatmap')\n",
        "plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
        "plt.yticks(range(len(tokens)), tokens)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5VgULswaXY6",
        "outputId": "429dea3f-c610-446b-9e22-677dfba93fc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 9, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "inputs[\"attention_mask\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI6jDPGZORXU"
      },
      "source": [
        "Great! That's working, now let's make sure that masking also works when we send in a 4-d tensor. Where dimension 0 is the layer number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-0xre52RORXU"
      },
      "outputs": [],
      "source": [
        "inputs[\"attention_mask\"] = inputs[\"attention_mask\"].unsqueeze(0).repeat(12,1,1,1)\n",
        "output_layerwise_mask = custom_model(**inputs, output_attentions=True)\n",
        "assert torch.allclose(output.attentions[0], output_layerwise_mask.attentions[0]), \"Attention scores have to be the same for the same attention mask even if it is repeated layerwise\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIdqwo3JORXU"
      },
      "source": [
        "## Creating Layerwise Attention Masks\n",
        "The `get_layerwise_attention_mask` function is designed to create a layer-wise attention mask from a given input attention mask. This function is particularly useful when working with transformer models like BERT, where different layers might require different attention masks.\n",
        "\n",
        "Here are the parameters of the function:\n",
        "\n",
        "- `attention_mask` (Tensor): This is the input attention mask that you want to convert into a layer-wise attention mask. It should be a tensor.\n",
        "\n",
        "- `pad_token_pos` (int): This is the position of the padding token in the attention mask. The function uses this to set the range of full attention.\n",
        "\n",
        "- `total_num_layers` (int, optional): This is the total number of layers in the model. By default, it's set to 12, which is the number of layers in BERT-base models.\n",
        "\n",
        "- `full_attention_layers` (list, optional): This is a list of layer numbers that should have full attention. By default, it's an empty list, meaning no layers have full attention.\n",
        "\n",
        "The function returns a tensor which is the layer-wise attention mask. If `full_attention_layers` is not empty, the function repeats the attention mask for each layer and sets full attention for the specified layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zFflnnXJu_7B"
      },
      "outputs": [],
      "source": [
        "def get_layerwise_attention_mask(attention_mask: Tensor, pad_token_pos:int, total_num_layers:int=12, full_attention_layers=[]):\n",
        "  \"\"\"\n",
        "  Returns a layer-wise attention mask for a given input attention mask.\n",
        "\n",
        "  Args:\n",
        "    attention_mask (Tensor): The input attention mask.\n",
        "    pad_token_pos (int): The position of the padding token in the attention mask.\n",
        "    total_num_layers (int, optional): The total number of layers in the model. Defaults to 12.\n",
        "    full_attention_layers (list, optional): A list of layer numbers that should have full attention. Defaults to [].\n",
        "\n",
        "  Returns:\n",
        "    Tensor: The layer-wise attention mask.\n",
        "  \"\"\"\n",
        "  attention_mask = attention_mask.unsqueeze(0)\n",
        "  if full_attention_layers:\n",
        "    attention_mask = attention_mask.repeat(total_num_layers, 1, 1, 1)\n",
        "    for layer_num in full_attention_layers:\n",
        "      attention_mask[layer_num, :, : pad_token_pos, : pad_token_pos] = 1\n",
        "  return attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIw-_d_PORXU"
      },
      "source": [
        "Let's make sure that this is working as desired.\n",
        "This test case is validating the `get_layerwise_attention_mask` function. It checks three things:\n",
        "\n",
        "1. The function returns an attention mask of the correct shape (12, 1, 4, 4).\n",
        "2. The attention mask at layer 0 is the same as the input mask (all zeros).\n",
        "3. The attention mask at layer 2 is all ones, indicating full attention.\n",
        "\n",
        "If any of these conditions are not met, the test case fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0M2fGF1nv4Lr"
      },
      "outputs": [],
      "source": [
        "dummy_attn_mask = torch.zeros(1, 4, 4)\n",
        "layerwise_attn_mask = get_layerwise_attention_mask(dummy_attn_mask, 4, full_attention_layers=[2])\n",
        "assert layerwise_attn_mask.shape == (12, 1, 4, 4), f\"{layerwise_attn_mask.shape} is the wrong shape\"\n",
        "assert torch.allclose(layerwise_attn_mask[0], dummy_attn_mask), \"Masks at layer 0 should all be 0s\"\n",
        "assert torch.allclose(layerwise_attn_mask[2], torch.ones(1,4,4)), \"Masks at layer 1 should all be 1s\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyRgM3FjORXV"
      },
      "source": [
        "## Padding custom attention Masks\n",
        "\n",
        "The `custom_collate` function is a utility function designed to process a batch of data for training a transformer model like BERT. It performs several key tasks:\n",
        "\n",
        "1. **Padding**: It pads all sequences in the batch to the same length, using the provided `pad_token_id`.\n",
        "\n",
        "2. **Layer-wise Attention Mask**: It creates a layer-wise attention mask for each sequence in the batch. This is useful when different layers of the transformer model require different attention masks.\n",
        "\n",
        "3. **Masked Language Modeling**: If the `is_mlm` flag is set to `True`, it applies masked language modeling to the input sequences. This involves randomly masking some of the tokens in each sequence, with the goal of predicting the original token from its context. The probability of masking a token is controlled by the `mlm_probability` parameter.\n",
        "\n",
        "The function returns a dictionary containing the padded 'input_ids', the layer-wise 'attention_mask', 'token_type_ids', and 'labels' (if present in the batch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Zh1P5ttKfwgy"
      },
      "outputs": [],
      "source": [
        "def custom_collate(batch:List, pad_token_id:int, full_attention_layers:List=[], mlm_probability:float=0.15, is_mlm:bool=False):\n",
        "  \"\"\"\n",
        "  Custom collate function for DataLoader that pads sequences and creates layer-wise attention masks.\n",
        "\n",
        "  Args:\n",
        "    batch (list): List of dictionaries containing 'input_ids', 'attention_mask', 'token_type_ids', and optionally 'label'.\n",
        "    pad_token_id (int): The ID of the padding token.\n",
        "    full_attention_layers (list, optional): List of layers that should have full attention. Defaults to [].\n",
        "    mlm_probability (float, optional): Probability of masking a token for masked language modeling. Defaults to 0.15.\n",
        "    is_mlm (bool, optional): Whether to apply masked language modeling. Defaults to False.\n",
        "\n",
        "  Returns:\n",
        "    dict: A dictionary containing 'input_ids', 'attention_mask', 'token_type_ids', and 'labels' (if present in batch).\n",
        "  \"\"\"\n",
        "  input_ids = [torch.LongTensor(batch[i][\"input_ids\"]) for i in range(len(batch))]\n",
        "  attention_mask = [torch.LongTensor(batch[i][\"attention_mask\"]) for i in range(len(batch))]\n",
        "  token_type_ids = [torch.LongTensor(batch[i][\"token_type_ids\"]) for i in range(len(batch))]\n",
        "  if \"label\" in batch[0]:\n",
        "    label = torch.LongTensor([batch[i][\"label\"] for i in range(len(batch))])\n",
        "  #idx = [batch[i][\"idx\"] for i in range(len(batch))]\n",
        "  max_len = max([len(inp) for inp in input_ids])\n",
        "  padding_sizes = [max_len - len(inp) for inp in input_ids]\n",
        "  input_ids = pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n",
        "  token_type_ids = pad_sequence(token_type_ids, batch_first=True)\n",
        "  attention_mask = [get_layerwise_attention_mask(pad(attention_mask[i], (0, padding_sizes[i], 0, padding_sizes[i]), value=0), padding_sizes[i], full_attention_layers=full_attention_layers).squeeze(1) for i in range(len(batch))]\n",
        "  attention_mask = torch.stack(attention_mask)\n",
        "  if full_attention_layers:\n",
        "    attention_mask = attention_mask.permute(1,0,2,3)\n",
        "  else:\n",
        "    attention_mask = attention_mask.squeeze(1)\n",
        "\n",
        "  if is_mlm:\n",
        "    input_ids, label = torch_mask_tokens(tokenizer, mlm_probability, input_ids)\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"labels\": label,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPjtSFNUVD-5",
        "outputId": "4f62153f-1552-4a5c-a00c-88a6ed268f4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  1,   2,   3,   4,   0],\n",
              "         [103,   2,   3,   4,   5]]),\n",
              " 'attention_mask': tensor([[[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]],\n",
              " \n",
              " \n",
              "         [[[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [0, 0, 0, 0, 0]],\n",
              " \n",
              "          [[1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0],\n",
              "           [1, 1, 1, 0, 0]]]]),\n",
              " 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0]]),\n",
              " 'labels': tensor([[-100, -100, -100, -100, -100],\n",
              "         [   1, -100, -100, -100, -100]])}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "custom_collate([{\"input_ids\":[1,2,3,4], \"attention_mask\":[[1,1,1,0],[1,1,1,0],[1,1,1,0],[1,1,1,0]], \"token_type_ids\":[0,0,0,0], \"label\":1}, {\"input_ids\":[1,2,3,4,5], \"attention_mask\":[[1,1,1,0,0],[1,1,1,0,0],[1,1,1,0,0],[1,1,1,0,0],[1,1,1,0,0]], \"token_type_ids\":[0,0,0,0,0], \"label\":1}], tokenizer.pad_token_id, full_attention_layers=[2,4], is_mlm=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_2ZoHk5RJ9P",
        "outputId": "d2077be4-6801-4c1c-c098-150a599e2c33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 11916\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpL48wHbORXV"
      },
      "source": [
        "We need to encode the datasets for dense and sparse attention differently since we need to create custom attention masks for the latter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gYSzPIq4JNUW"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset, tokenizer, mode=\"sparse\", is_mlm:bool = False):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mode = mode\n",
        "        self.is_mlm = is_mlm\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if self.mode == \"sparse\":\n",
        "        return custom_tokenize(self.tokenizer, self.dataset[idx][\"text\"])\n",
        "      else:\n",
        "        inputs = tokenizer(self.dataset[idx][\"text\"], return_tensors=\"pt\", truncation=True)\n",
        "        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
        "        if not self.is_mlm:\n",
        "          inputs[\"labels\"] = self.dataset[idx][\"label\"]\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u36vJQsnQyR9"
      },
      "outputs": [],
      "source": [
        "train_datasets_dense_attention, dev_datasets_dense_attention, test_datasets_dense_attention = [], [], []\n",
        "train_datasets_sparse_attention, dev_datasets_sparse_attention, test_datasets_sparse_attention = [], [], []\n",
        "for i in range(len(train_datasets)):\n",
        "    # Encode the datasets that'll leverage dense attention\n",
        "    train_datasets_dense_attention.append(CustomDataset(train_datasets[i], tokenizer, mode=\"dense\"))\n",
        "    dev_datasets_dense_attention.append(CustomDataset(dev_datasets[i], tokenizer, mode=\"dense\"))\n",
        "    test_datasets_dense_attention.append(CustomDataset(test_datasets[i], tokenizer, mode=\"dense\"))\n",
        "\n",
        "    # Encode the datasets that'll leverage sparse attention\n",
        "    train_datasets_sparse_attention.append(CustomDataset(train_datasets[i], tokenizer, mode=\"sparse\"))\n",
        "    dev_datasets_sparse_attention.append(CustomDataset(dev_datasets[i], tokenizer, mode=\"sparse\"))\n",
        "    test_datasets_sparse_attention.append(CustomDataset(test_datasets[i], tokenizer, mode=\"sparse\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasets[1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-g-7SVtSs7O",
        "outputId": "e238b031-92a3-4113-8e34-cc85e197f989"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
              " 'label': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasets_size"
      ],
      "metadata": {
        "id": "OAD5ncoLiw5q",
        "outputId": "c1daedb1-c915-4f14-ef3a-3341c7231f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16000, 30000, 11916]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trPzkAwFORXW"
      },
      "source": [
        "## Training the model with dense attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FuAngW-DJNUW"
      },
      "outputs": [],
      "source": [
        "logging.set_verbosity_warning()\n",
        "\n",
        "def train_and_evaluate(\n",
        "    train_datasets: List[Dataset],\n",
        "    dev_datasets: List[Dataset],\n",
        "    test_datasets: List[Dataset],\n",
        "    tokenizer: AutoTokenizer,\n",
        "    train_batch_size:int = 64,\n",
        "    num_epochs:int = 5\n",
        ") -> Dict[str, List[Dict]]:\n",
        "    results = defaultdict(list)\n",
        "    # Define the training arguments\n",
        "    for i in range(len(train_datasets)):\n",
        "        total_num_steps = (num_epochs * train_datasets_size[i])//train_batch_size\n",
        "        print(f\"Training on {dataset_names[i][0]}\")\n",
        "        for j in range(5):\n",
        "            print(f\"Training for the {j}th run.\")\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels[i])\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir='./results',          # output directory\n",
        "                num_train_epochs=5,            # total number of training steps\n",
        "                per_device_train_batch_size=train_batch_size,  # batch size per device during training\n",
        "                per_device_eval_batch_size=128,   # batch size for evaluation\n",
        "                warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\n",
        "                weight_decay=0.01,               # strength of weight decay\n",
        "                logging_dir='./logs',            # directory for storing logs\n",
        "                fp16=True,\n",
        "                seed=j,\n",
        "                gradient_checkpointing=True,\n",
        "                evaluation_strategy=\"steps\",\n",
        "                eval_steps=total_num_steps//10,\n",
        "                load_best_model_at_end=True,\n",
        "                logging_steps=total_num_steps//10,\n",
        "                save_steps=total_num_steps//10,\n",
        "                use_cpu=False\n",
        "            )\n",
        "\n",
        "            # Initialize the trainer\n",
        "            trainer_dense_attention = Trainer(\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,                 # the instantiated 🤗 Transformers model to be trained\n",
        "                args=training_args,                  # training arguments, defined above\n",
        "                train_dataset=train_datasets[i],         # training dataset\n",
        "                eval_dataset=dev_datasets[i],       # evaluation dataset\n",
        "                data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=True),\n",
        "                compute_metrics=compute_metrics\n",
        "        )\n",
        "            trainer_dense_attention.train()\n",
        "            test_results = trainer_dense_attention.evaluate(test_datasets[i])\n",
        "            print(f\"Results on the test set are: {test_results}\")\n",
        "            results[dataset_names[i]].append(test_results)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4v4QFQumB6B-",
        "outputId": "84f4971a-f98d-493b-8ba4-e735e9850f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on dair-ai/emotion\n",
            "Training for the 0th run.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 03:07, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.224300</td>\n",
              "      <td>0.430696</td>\n",
              "      <td>0.876000</td>\n",
              "      <td>0.836445</td>\n",
              "      <td>0.847957</td>\n",
              "      <td>0.828568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.298800</td>\n",
              "      <td>0.196809</td>\n",
              "      <td>0.926500</td>\n",
              "      <td>0.902691</td>\n",
              "      <td>0.887671</td>\n",
              "      <td>0.921164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.164400</td>\n",
              "      <td>0.175495</td>\n",
              "      <td>0.931500</td>\n",
              "      <td>0.909727</td>\n",
              "      <td>0.899409</td>\n",
              "      <td>0.929152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.146900</td>\n",
              "      <td>0.138754</td>\n",
              "      <td>0.933500</td>\n",
              "      <td>0.907027</td>\n",
              "      <td>0.921483</td>\n",
              "      <td>0.898464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.098500</td>\n",
              "      <td>0.145892</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.911888</td>\n",
              "      <td>0.911896</td>\n",
              "      <td>0.914970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.096000</td>\n",
              "      <td>0.148460</td>\n",
              "      <td>0.942000</td>\n",
              "      <td>0.918825</td>\n",
              "      <td>0.935196</td>\n",
              "      <td>0.908875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>0.074800</td>\n",
              "      <td>0.149594</td>\n",
              "      <td>0.942500</td>\n",
              "      <td>0.920623</td>\n",
              "      <td>0.912975</td>\n",
              "      <td>0.929802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.072200</td>\n",
              "      <td>0.147248</td>\n",
              "      <td>0.938500</td>\n",
              "      <td>0.911015</td>\n",
              "      <td>0.921561</td>\n",
              "      <td>0.901394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>0.051300</td>\n",
              "      <td>0.160947</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.913857</td>\n",
              "      <td>0.914683</td>\n",
              "      <td>0.914397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.049300</td>\n",
              "      <td>0.161354</td>\n",
              "      <td>0.937000</td>\n",
              "      <td>0.909679</td>\n",
              "      <td>0.913461</td>\n",
              "      <td>0.906075</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on the test set are: {'eval_loss': 0.13807015120983124, 'eval_accuracy': 0.936, 'eval_f1': 0.8918817242216576, 'eval_precision': 0.916948905143263, 'eval_recall': 0.882673481802767, 'eval_runtime': 1.5035, 'eval_samples_per_second': 1330.271, 'eval_steps_per_second': 10.642, 'epoch': 5.0}\n",
            "Training for the 1th run.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 03:15, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.276100</td>\n",
              "      <td>0.535789</td>\n",
              "      <td>0.839000</td>\n",
              "      <td>0.758293</td>\n",
              "      <td>0.813194</td>\n",
              "      <td>0.744162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.314500</td>\n",
              "      <td>0.217951</td>\n",
              "      <td>0.922000</td>\n",
              "      <td>0.894736</td>\n",
              "      <td>0.910988</td>\n",
              "      <td>0.889773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.168200</td>\n",
              "      <td>0.165878</td>\n",
              "      <td>0.933500</td>\n",
              "      <td>0.907832</td>\n",
              "      <td>0.906618</td>\n",
              "      <td>0.913930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.144200</td>\n",
              "      <td>0.147249</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.909846</td>\n",
              "      <td>0.937193</td>\n",
              "      <td>0.892845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.100800</td>\n",
              "      <td>0.141843</td>\n",
              "      <td>0.935000</td>\n",
              "      <td>0.910784</td>\n",
              "      <td>0.895754</td>\n",
              "      <td>0.931220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.098100</td>\n",
              "      <td>0.123331</td>\n",
              "      <td>0.940500</td>\n",
              "      <td>0.914670</td>\n",
              "      <td>0.917762</td>\n",
              "      <td>0.912337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>0.071100</td>\n",
              "      <td>0.146867</td>\n",
              "      <td>0.930500</td>\n",
              "      <td>0.900385</td>\n",
              "      <td>0.912310</td>\n",
              "      <td>0.893599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.068700</td>\n",
              "      <td>0.145530</td>\n",
              "      <td>0.937000</td>\n",
              "      <td>0.910488</td>\n",
              "      <td>0.909640</td>\n",
              "      <td>0.911647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>0.049500</td>\n",
              "      <td>0.180716</td>\n",
              "      <td>0.933000</td>\n",
              "      <td>0.904476</td>\n",
              "      <td>0.910723</td>\n",
              "      <td>0.899142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.045800</td>\n",
              "      <td>0.182240</td>\n",
              "      <td>0.932500</td>\n",
              "      <td>0.904921</td>\n",
              "      <td>0.909612</td>\n",
              "      <td>0.900762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on the test set are: {'eval_loss': 0.1356077343225479, 'eval_accuracy': 0.9335, 'eval_f1': 0.8850426414566795, 'eval_precision': 0.8872851926238968, 'eval_recall': 0.8831655366743235, 'eval_runtime': 1.584, 'eval_samples_per_second': 1262.598, 'eval_steps_per_second': 10.101, 'epoch': 5.0}\n",
            "Training for the 2th run.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 03:16, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.252100</td>\n",
              "      <td>0.475360</td>\n",
              "      <td>0.851500</td>\n",
              "      <td>0.766334</td>\n",
              "      <td>0.835422</td>\n",
              "      <td>0.753433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.315900</td>\n",
              "      <td>0.201462</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.899861</td>\n",
              "      <td>0.889581</td>\n",
              "      <td>0.912073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.166500</td>\n",
              "      <td>0.176434</td>\n",
              "      <td>0.933500</td>\n",
              "      <td>0.905104</td>\n",
              "      <td>0.905798</td>\n",
              "      <td>0.908446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.142800</td>\n",
              "      <td>0.145231</td>\n",
              "      <td>0.937000</td>\n",
              "      <td>0.912968</td>\n",
              "      <td>0.928599</td>\n",
              "      <td>0.907723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.101800</td>\n",
              "      <td>0.145261</td>\n",
              "      <td>0.941000</td>\n",
              "      <td>0.917388</td>\n",
              "      <td>0.915613</td>\n",
              "      <td>0.921458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.096700</td>\n",
              "      <td>0.138621</td>\n",
              "      <td>0.937000</td>\n",
              "      <td>0.908918</td>\n",
              "      <td>0.926078</td>\n",
              "      <td>0.895588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>0.073500</td>\n",
              "      <td>0.135416</td>\n",
              "      <td>0.940500</td>\n",
              "      <td>0.919067</td>\n",
              "      <td>0.908087</td>\n",
              "      <td>0.931780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.078700</td>\n",
              "      <td>0.143361</td>\n",
              "      <td>0.939000</td>\n",
              "      <td>0.913432</td>\n",
              "      <td>0.911681</td>\n",
              "      <td>0.916184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>0.051900</td>\n",
              "      <td>0.162711</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>0.912847</td>\n",
              "      <td>0.914314</td>\n",
              "      <td>0.912796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.051900</td>\n",
              "      <td>0.162806</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.914431</td>\n",
              "      <td>0.911517</td>\n",
              "      <td>0.917704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on the test set are: {'eval_loss': 0.14952251315116882, 'eval_accuracy': 0.9305, 'eval_f1': 0.8893978747451242, 'eval_precision': 0.8754485732155328, 'eval_recall': 0.906653211100224, 'eval_runtime': 1.5185, 'eval_samples_per_second': 1317.117, 'eval_steps_per_second': 10.537, 'epoch': 5.0}\n",
            "Training for the 3th run.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='626' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 626/1250 01:32 < 01:32, 6.74 it/s, Epoch 2.50/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.278600</td>\n",
              "      <td>0.418202</td>\n",
              "      <td>0.885500</td>\n",
              "      <td>0.845803</td>\n",
              "      <td>0.862217</td>\n",
              "      <td>0.832263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.295700</td>\n",
              "      <td>0.218858</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.906732</td>\n",
              "      <td>0.918903</td>\n",
              "      <td>0.905279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.170100</td>\n",
              "      <td>0.172514</td>\n",
              "      <td>0.929000</td>\n",
              "      <td>0.903991</td>\n",
              "      <td>0.896464</td>\n",
              "      <td>0.917108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.137900</td>\n",
              "      <td>0.139188</td>\n",
              "      <td>0.936500</td>\n",
              "      <td>0.908582</td>\n",
              "      <td>0.917100</td>\n",
              "      <td>0.901720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.099600</td>\n",
              "      <td>0.151266</td>\n",
              "      <td>0.936500</td>\n",
              "      <td>0.914540</td>\n",
              "      <td>0.901144</td>\n",
              "      <td>0.933351</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dense_attention_results = train_and_evaluate(train_datasets_dense_attention, dev_datasets_dense_attention, test_datasets_dense_attention, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dense_attention_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "lx0QsLmJY6fg",
        "outputId": "dad5ae88-16c7-46ff-81a5-a3d7f65f397e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-e2853a1e5f44>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_attention_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dense_attention_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zykybsCQORXW"
      },
      "source": [
        "## Evaluating the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "p7kiaNeRV2Ze",
        "outputId": "16ddda19-9be8-4918-b6b7-2c16ee384ce2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.14260536432266235,\n",
              " 'eval_accuracy': 0.9305,\n",
              " 'eval_f1': 0.8869114759799867,\n",
              " 'eval_precision': 0.8952005188725458,\n",
              " 'eval_recall': 0.8813286392626377,\n",
              " 'eval_runtime': 0.779,\n",
              " 'eval_samples_per_second': 2567.461,\n",
              " 'eval_steps_per_second': 20.54,\n",
              " 'epoch': 5.0}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer_dense_attention.evaluate(test_dataset_dense_attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFds-DiEORXW",
        "outputId": "446078ae-7f6c-444f-a4a7-b080a90a4bd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "## Training the model with sparse attention\n",
        "custom_model = CustomBertForSequenceClassification.from_pretrained(model_name, num_labels=len(train_dataset.features[\"label\"].names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hjzZIuMS9UNz",
        "outputId": "faaaff45-5437-4bb8-9a50-275b83ad7802"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 03:26, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.453700</td>\n",
              "      <td>0.859466</td>\n",
              "      <td>0.697000</td>\n",
              "      <td>0.448461</td>\n",
              "      <td>0.653476</td>\n",
              "      <td>0.469717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.483600</td>\n",
              "      <td>0.297570</td>\n",
              "      <td>0.909500</td>\n",
              "      <td>0.882669</td>\n",
              "      <td>0.878018</td>\n",
              "      <td>0.888713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.248500</td>\n",
              "      <td>0.231922</td>\n",
              "      <td>0.911500</td>\n",
              "      <td>0.877822</td>\n",
              "      <td>0.887640</td>\n",
              "      <td>0.869651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.197700</td>\n",
              "      <td>0.189323</td>\n",
              "      <td>0.928500</td>\n",
              "      <td>0.903730</td>\n",
              "      <td>0.921616</td>\n",
              "      <td>0.893146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.174000</td>\n",
              "      <td>0.170435</td>\n",
              "      <td>0.926500</td>\n",
              "      <td>0.900691</td>\n",
              "      <td>0.911592</td>\n",
              "      <td>0.891724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.113700</td>\n",
              "      <td>0.171035</td>\n",
              "      <td>0.925000</td>\n",
              "      <td>0.893360</td>\n",
              "      <td>0.889176</td>\n",
              "      <td>0.898020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.120300</td>\n",
              "      <td>0.172024</td>\n",
              "      <td>0.927000</td>\n",
              "      <td>0.904048</td>\n",
              "      <td>0.890522</td>\n",
              "      <td>0.921063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.098600</td>\n",
              "      <td>0.166487</td>\n",
              "      <td>0.932000</td>\n",
              "      <td>0.904231</td>\n",
              "      <td>0.899451</td>\n",
              "      <td>0.909561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.081500</td>\n",
              "      <td>0.180832</td>\n",
              "      <td>0.928500</td>\n",
              "      <td>0.903243</td>\n",
              "      <td>0.901719</td>\n",
              "      <td>0.905511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.076800</td>\n",
              "      <td>0.188413</td>\n",
              "      <td>0.930500</td>\n",
              "      <td>0.903084</td>\n",
              "      <td>0.917928</td>\n",
              "      <td>0.890809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.063200</td>\n",
              "      <td>0.190553</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.904714</td>\n",
              "      <td>0.901623</td>\n",
              "      <td>0.909829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.052300</td>\n",
              "      <td>0.194049</td>\n",
              "      <td>0.932500</td>\n",
              "      <td>0.905835</td>\n",
              "      <td>0.920813</td>\n",
              "      <td>0.893755</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1250, training_loss=0.2550808616638184, metrics={'train_runtime': 206.7186, 'train_samples_per_second': 386.999, 'train_steps_per_second': 6.047, 'total_flos': 2323386947729664.0, 'train_loss': 0.2550808616638184, 'epoch': 5.0})"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer_sparse_attention = Trainer(\n",
        "    model=custom_model,\n",
        "    tokenizer=tokenizer,                 # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset_sparse_attention,         # training dataset\n",
        "    eval_dataset=dev_dataset_sparse_attention,           # evaluation dataset\n",
        "    data_collator=partial(custom_collate, pad_token_id=tokenizer.pad_token_id),\n",
        "    compute_metrics=compute_metrics\n",
        "  )\n",
        "\n",
        "trainer_sparse_attention.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vcH1lFkORXe"
      },
      "source": [
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "0Maf7a4gD_pP",
        "outputId": "c279018a-1a9e-4fa4-c86b-fba527398961"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.17503787577152252,\n",
              " 'eval_accuracy': 0.924,\n",
              " 'eval_f1': 0.881268107518956,\n",
              " 'eval_precision': 0.8770890771932288,\n",
              " 'eval_recall': 0.8859999660070658,\n",
              " 'eval_runtime': 1.5774,\n",
              " 'eval_samples_per_second': 1267.898,\n",
              " 'eval_steps_per_second': 10.143,\n",
              " 'epoch': 5.0}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer_sparse_attention.evaluate(test_dataset_sparse_attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnlAdT1RORXe"
      },
      "source": [
        "## Training a model with hybrid dense+sparse attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SritogvJ1-PL",
        "outputId": "bbace690-b2ca-4bb6-fbbe-6a5b722318f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 03:47, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.491700</td>\n",
              "      <td>0.867480</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.442549</td>\n",
              "      <td>0.594815</td>\n",
              "      <td>0.461187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.489000</td>\n",
              "      <td>0.274168</td>\n",
              "      <td>0.911500</td>\n",
              "      <td>0.882613</td>\n",
              "      <td>0.880070</td>\n",
              "      <td>0.885990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.242000</td>\n",
              "      <td>0.221881</td>\n",
              "      <td>0.921000</td>\n",
              "      <td>0.895113</td>\n",
              "      <td>0.898978</td>\n",
              "      <td>0.895241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.186800</td>\n",
              "      <td>0.184280</td>\n",
              "      <td>0.928500</td>\n",
              "      <td>0.903116</td>\n",
              "      <td>0.916973</td>\n",
              "      <td>0.893337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.172200</td>\n",
              "      <td>0.146533</td>\n",
              "      <td>0.938500</td>\n",
              "      <td>0.913183</td>\n",
              "      <td>0.925667</td>\n",
              "      <td>0.903153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.111600</td>\n",
              "      <td>0.165268</td>\n",
              "      <td>0.932500</td>\n",
              "      <td>0.904286</td>\n",
              "      <td>0.911461</td>\n",
              "      <td>0.900496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.110700</td>\n",
              "      <td>0.147053</td>\n",
              "      <td>0.929000</td>\n",
              "      <td>0.901380</td>\n",
              "      <td>0.907041</td>\n",
              "      <td>0.896373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.092100</td>\n",
              "      <td>0.142600</td>\n",
              "      <td>0.935500</td>\n",
              "      <td>0.910201</td>\n",
              "      <td>0.908313</td>\n",
              "      <td>0.913224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.080300</td>\n",
              "      <td>0.148731</td>\n",
              "      <td>0.930500</td>\n",
              "      <td>0.903956</td>\n",
              "      <td>0.905733</td>\n",
              "      <td>0.903091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.074800</td>\n",
              "      <td>0.157338</td>\n",
              "      <td>0.934500</td>\n",
              "      <td>0.906104</td>\n",
              "      <td>0.914557</td>\n",
              "      <td>0.899937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.060800</td>\n",
              "      <td>0.160194</td>\n",
              "      <td>0.941500</td>\n",
              "      <td>0.918647</td>\n",
              "      <td>0.913848</td>\n",
              "      <td>0.926273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.058200</td>\n",
              "      <td>0.164922</td>\n",
              "      <td>0.934500</td>\n",
              "      <td>0.908300</td>\n",
              "      <td>0.919229</td>\n",
              "      <td>0.899281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1250, training_loss=0.2558851728439331, metrics={'train_runtime': 227.2943, 'train_samples_per_second': 351.967, 'train_steps_per_second': 5.499, 'total_flos': 2323386947729664.0, 'train_loss': 0.2558851728439331, 'epoch': 5.0})"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "custom_model = CustomBertForSequenceClassification.from_pretrained(model_name, num_labels=len(train_dataset.features[\"label\"].names))\n",
        "trainer_sparse_layerwise_attention = Trainer(\n",
        "    model=custom_model,\n",
        "    tokenizer=tokenizer,                 # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset_sparse_attention,         # training dataset\n",
        "    eval_dataset=dev_dataset_sparse_attention,           # evaluation dataset\n",
        "    data_collator=partial(custom_collate, pad_token_id=tokenizer.pad_token_id, full_attention_layers=[0,1,2,3]),\n",
        "    compute_metrics=compute_metrics\n",
        "  )\n",
        "\n",
        "trainer_sparse_layerwise_attention.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "p7wWn6Bm5oTn",
        "outputId": "b1ff649d-e354-46a0-ec33-36bb1be047e5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.14260034263134003,\n",
              " 'eval_accuracy': 0.9355,\n",
              " 'eval_f1': 0.9102013880474451,\n",
              " 'eval_precision': 0.9083133101100925,\n",
              " 'eval_recall': 0.9132236399814221,\n",
              " 'eval_runtime': 1.8579,\n",
              " 'eval_samples_per_second': 1076.493,\n",
              " 'eval_steps_per_second': 8.612,\n",
              " 'epoch': 5.0}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer_sparse_layerwise_attention.evaluate(test_dataset_sparse_attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_X10Kq0ddln"
      },
      "source": [
        "## BERT Pre-training with Sparse Attention\n",
        "\n",
        "Let's pre-train a Bert model with sparse attention to validate if pre-training with sparse attention gives a boost to finetuning when using the same strategy rather than solely finetuning with sparse attention.\n",
        "\n",
        "We'll use the **openwebtext** dataset for pre-training, the original model wasn't pre-trained on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpk9ss5CVD-8"
      },
      "outputs": [],
      "source": [
        "def get_pretraining_dataset(dataset_name):\n",
        "    ds = load_dataset(dataset_name, split='train', streaming=True)\n",
        "    return ds.take(136000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP-UME59ORXe"
      },
      "source": [
        "Since the dataset is too large to load into memory, we'll use the streaming API to load a subset of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIt2jspSXWIC"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from datasets import Dataset\n",
        "\n",
        "def gen_from_iterable_dataset(iterable_ds):\n",
        "    yield from iterable_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMr1yCxSVD-8"
      },
      "outputs": [],
      "source": [
        "pretraining_train_dataset = get_pretraining_dataset(\"Skylion007/openwebtext\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PufAnAMIORXf"
      },
      "source": [
        "Convert an `IterableDataset` to `Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNfz66C-Xf78"
      },
      "outputs": [],
      "source": [
        "pretraining_train_dataset = Dataset.from_generator(partial(gen_from_iterable_dataset, pretraining_train_dataset), features=pretraining_train_dataset.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikDNn8zdK4qu"
      },
      "outputs": [],
      "source": [
        "pretraining_train_dataset = pretraining_train_dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3hZW_PrLNMp"
      },
      "outputs": [],
      "source": [
        "pretraining_dev_dataset = pretraining_train_dataset[\"test\"]\n",
        "pretraining_train_dataset = pretraining_train_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjJZl8j4ORXf"
      },
      "source": [
        "### Optional: Save the dataset to memory for faster loading in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "D_sKiTa-35r7",
        "outputId": "8d132cbb-2225-446c-aab7-f0c099d077fc"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ca1a8b7047ee>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretraining_train_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./datasets/pretraining_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpretraining_dev_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./datasets/pretraining_dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pretraining_train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "pretraining_train_dataset.save_to_disk(\"./datasets/pretraining_train\")\n",
        "pretraining_dev_dataset.save_to_disk(\"./datasets/pretraining_dev\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBcFcwwq35r7"
      },
      "outputs": [],
      "source": [
        "pretraining_train_dataset = load_from_disk(\"./datasets/pretraining_train\")\n",
        "pretraining_dev_dataset = load_from_disk(\"./datasets/pretraining_dev\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULTf7G4AVD-8",
        "outputId": "c64ece6e-3e40-40c5-8258-3fded0c3f1f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 122400\n",
              "})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretraining_train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qncBl_WqYUnJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# pretraining_train_dataset = pretraining_train_dataset.map(lambda example: custom_tokenize(tokenizer, example[\"text\"]), batched=False)\n",
        "# pretraining_dev_dataset = pretraining_dev_dataset.map(lambda example: custom_tokenize(tokenizer, example[\"text\"]), batched=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPuq8rvNORXf"
      },
      "source": [
        "Since the dataset is large we don't want to vectorize all of it into memory as we did for our downstream tasks. We'll create a custom Pytorch Dataset that vectorizes only the required items of the batch into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYwoZ1-u35r8",
        "outputId": "a1f4b333-ada6-40e9-b91a-7143c3ca2579"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pretraining_train_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[65], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n\u001b[1;32m---> 19\u001b[0m pretraining_train_torch_dataset \u001b[38;5;241m=\u001b[39m PretrainingDataset(\u001b[43mpretraining_train_dataset\u001b[49m, tokenizer)\n\u001b[0;32m     20\u001b[0m pretraining_dev_torch_dataset \u001b[38;5;241m=\u001b[39m PretrainingDataset(pretraining_dev_dataset, tokenizer)\n\u001b[0;32m     22\u001b[0m pretraining_train_torch_dense_dataset \u001b[38;5;241m=\u001b[39m PretrainingDataset(pretraining_train_dataset, tokenizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pretraining_train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "pretraining_train_torch_dataset = CustomDataset(pretraining_train_dataset, tokenizer)\n",
        "pretraining_dev_torch_dataset = CustomDataset(pretraining_dev_dataset, tokenizer)\n",
        "\n",
        "pretraining_train_torch_dense_dataset = CustomDataset(pretraining_train_dataset, tokenizer, mode=\"dense\")\n",
        "pretraining_dev_torch_dense_dataset = CustomDataset(pretraining_dev_dataset, tokenizer, mode=\"dense\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kB0H56_ORXf"
      },
      "source": [
        "We'll continue pre-training the original Bert Model using a Masked Language Modeling head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d69Tx1ud-YN3",
        "outputId": "bd4363e8-05c0-450b-856a-df876d92cf7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "mlm_model = BertForMaskedLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9Fh-iENBHqu",
        "outputId": "dd3d4241-995a-48cc-cee3-10f6472aed00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[101, 7592, 102]"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y11X8-BqORXg"
      },
      "source": [
        "We'll train for 500 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U27PvFr8VD-8"
      },
      "outputs": [],
      "source": [
        "pretraining_args = TrainingArguments(\n",
        "    output_dir='./bert-sparse-sliding-window-attention',          # output directory\n",
        "    max_steps=500,            # total number of training steps\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    fp16=True,\n",
        "    #torch_compile=True, # optimizations\n",
        "    #optim=\"adamw_torch_fused\", # improved optimizer\n",
        "    gradient_checkpointing=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=200,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=100,\n",
        "    eval_accumulation_steps=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    push_to_hub=True\n",
        ")\n",
        "\n",
        "pretrainer_dense_attention = Trainer(\n",
        "    model=mlm_model,\n",
        "    tokenizer=tokenizer,                 # the instantiated 🤗 Transformers model to be trained\n",
        "    args=pretraining_args,                  # training arguments, defined above\n",
        "    train_dataset=pretraining_train_torch_dense_dataset,         # training dataset\n",
        "    eval_dataset=pretraining_dev_torch_dense_dataset,           # evaluation dataset\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldss1YJAORXg"
      },
      "source": [
        "Let's examine the loss of the original Bert Model on the dev dataset. Note that this is the model that uses **dense attention**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "h6mbUl615DE1",
        "outputId": "688a1c90-1349-4c13-c627-8a5613e4d4fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [213/213 01:46]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 2.7092370986938477,\n",
              " 'eval_runtime': 107.939,\n",
              " 'eval_samples_per_second': 125.997,\n",
              " 'eval_steps_per_second': 1.973}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrainer_dense_attention.evaluate(pretraining_dev_torch_dense_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiIjinOgx_QA",
        "outputId": "707cae59-5a3d-4c66-e1f6-ef208a61642d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mWjQvqyORXg"
      },
      "source": [
        "Let's now create a CustomBertModel that uses the CustomBertEncoder that we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht2Bp2RTWHrm"
      },
      "outputs": [],
      "source": [
        "class CustomBertForMaskedLM(BertForMaskedLM):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.bert = CustomBertModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEt-_3hGWIF8"
      },
      "outputs": [],
      "source": [
        "custom_mlm_model = CustomBertForMaskedLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWPfr2FxWXcO"
      },
      "outputs": [],
      "source": [
        "pretrainer_sparse_layerwise_attention = Trainer(\n",
        "    model=custom_mlm_model,\n",
        "    tokenizer=tokenizer,                 # the instantiated 🤗 Transformers model to be trained\n",
        "    args=pretraining_args,                  # training arguments, defined above\n",
        "    train_dataset=pretraining_train_torch_dataset,         # training dataset\n",
        "    eval_dataset=pretraining_dev_torch_dataset,           # evaluation dataset\n",
        "    data_collator = partial(custom_collate, pad_token_id=tokenizer.pad_token_id, is_mlm=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "HVv7X6fc2bFC",
        "outputId": "a0c1684f-bc3d-4931-99ad-e9ac2c11633b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [213/213 01:58]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity before training : {'eval_loss': 4.9193434715271, 'eval_runtime': 119.7049, 'eval_samples_per_second': 113.613, 'eval_steps_per_second': 1.779}\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loss before training : {pretrainer_sparse_layerwise_attention.evaluate(pretraining_dev_torch_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "vLM-HsqzWwWb",
        "outputId": "661588cf-a583-410c-e6fa-cd4b2fed9382"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 31:48, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.634400</td>\n",
              "      <td>3.035876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.104800</td>\n",
              "      <td>2.889886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.012300</td>\n",
              "      <td>2.828573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.961000</td>\n",
              "      <td>2.797679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.939900</td>\n",
              "      <td>2.782165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='426' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [213/213 08:21]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=3.130488464355469, metrics={'train_runtime': 1911.784, 'train_samples_per_second': 33.477, 'train_steps_per_second': 0.262, 'total_flos': 1.6961223131136e+16, 'train_loss': 3.130488464355469, 'epoch': 0.52})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pretrainer_sparse_layerwise_attention.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "YTBwgQu2ElPH",
        "outputId": "140f7305-be70-4251-80d9-93836a87d2ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [213/213 01:59]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity after training : {'eval_loss': 2.8012802600860596, 'eval_runtime': 119.7757, 'eval_samples_per_second': 113.546, 'eval_steps_per_second': 1.778, 'epoch': 0.52}\n"
          ]
        }
      ],
      "source": [
        "print(f\"Perplexity after training : {pretrainer_sparse_layerwise_attention.evaluate(pretraining_dev_torch_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRmhjfwJYL9q"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLVA67z9Z8UL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}