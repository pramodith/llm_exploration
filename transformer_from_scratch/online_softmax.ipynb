{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5928549e",
   "metadata": {},
   "source": [
    "# Online Softmax\n",
    "Flash Attention has been responsible for cutting down the run-time of transformer models. Flash Attention can be broken down into two sets of optimizations:\n",
    "1. GPU Aware I/O Optimizations. These optimizations are related to how data is moved between GPU memory and on-chip (SRAM) memory. These optimizations are not discussed here.\n",
    "2. Online Softmax. This is an algorithmic optimization that allows us to compute softmax in chunks. Each chunk fits into the SRAM memory of the GPU and can be computed in parallel across multiple streaming multiprocessors (SMs).\n",
    "\n",
    "In this blog post we will discuss the online softmax algorithm and the simple mathematical tricks that makes it possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "29cbecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1211b10",
   "metadata": {},
   "source": [
    "## Softmax Formula\n",
    "\n",
    "Given a vector of scores $z = [z_1, z_2, \\ldots, z_n]$, the softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5fc9d09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3880e-01, 5.7845e-03, 5.3377e-07, 8.2416e-01, 1.1100e-02, 1.4832e-03,\n",
       "         1.8518e-02, 1.5404e-04]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = torch.randn(1, 8)\n",
    "keys = torch.randn(8, 8)\n",
    "\n",
    "dot_products = torch.matmul(query, keys.T)\n",
    "sofmax = nn.functional.softmax(dot_products, dim=-1)\n",
    "sofmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeef377",
   "metadata": {},
   "source": [
    "## Softmax Subtract By Max\n",
    "To avoid overflow implementations of softmax often subtract all the dot-products by the maximum value in the set. Mathematically this operation is equivalent to normal softmax because of the following:\n",
    "\n",
    "In the numerator and denominator we can factor out a constant \n",
    "$e^{-c}$, where $c = \\max(x)$:\n",
    "\n",
    "Numerator:\n",
    "$$\n",
    "e^{x - c} = e^{x} \\cdot e^{-c}\n",
    "$$\n",
    "\n",
    "Denominator:\n",
    "$$\n",
    "\\sum_{j=1}^{n} e^{x_j - c} = \\sum_{j=1}^{n} e^{x_j} \\cdot e^{-c}\n",
    "$$\n",
    "\n",
    "Thus we have:\n",
    "$$\n",
    "\\text{softmax}(x_i - c) = \\frac{e^{x_i - c}}{\\sum_{j=1}^{n} e^{x_j - c}} = \n",
    "\\frac{e^{x_i} \\cdot \\cancel{e^{-c}}}{\\sum_{j=1}^{n} e^{x_j} \\cdot \\cancel{e^{-c}}} = \n",
    "\\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} = \\text{softmax}(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ea52615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = torch.max(dot_products, dim=-1, keepdim=True)[0]\n",
    "dot_products -= maxs\n",
    "softmax_post_max = torch.nn.functional.softmax(dot_products, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286f4dc",
   "metadata": {},
   "source": [
    "Let's verify that these two softmax computations are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "167fbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(sofmax, softmax_post_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210e84b",
   "metadata": {},
   "source": [
    "## Online Softmax Numerator\n",
    "\n",
    "Now let's assume that we have limited memory and cannot compute all the dot-products at once. Instead we need to process them in the max number of chunks that fit in memory.\n",
    "\n",
    "But hold on, the subtraction by minimum trick relies on knowing the minimum value across all dot-products. If we only have access to a chunk of dot-products at a time, how can we compute the global minimum?\n",
    "\n",
    "We can compute the minimum in an online fashion by keeping track of the minimum value seen so far as we process each chunk. Let's assume that we have two chunks of dot-products:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{chunk}_1 &= [d_1, d_2, \\ldots, d_k] \\\\\n",
    "\\text{chunk\\_max}_1 &= \\max(\\text{chunk}_1) \\\\\n",
    "\\text{chunk\\_1\\_softmax} &= \\text{softmax}(\\text{chunk}_1 - \\text{chunk\\_max}_1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{chunk}_2 &= [d_{k+1}, d_{k+2}, \\ldots, d_n] \\\\\n",
    "\\text{chunk\\_max}_2 &= \\max(\\text{chunk}_2) \\\\\n",
    "\\text{chunk\\_2\\_softmax} &= \\text{softmax}(\\text{chunk}_2 - \\text{chunk\\_max}_2)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To compute the overall softmax, we need to adjust the chunk softmaxes based on the difference between the chunk minimums and the global minimum. We get to this by a little bit of high-school math:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "s &= s_0 + s_1 \\\\\n",
    "e^{s} &= e^{s_0 + s_1} = e^{s_0} \\cdot e^{s_1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The max can be re-written as:\n",
    "\n",
    "$$\n",
    "\\text{global\\_max} = \\max(\\text{chunk\\_max}_1, \\text{chunk\\_max}_2)\n",
    "$$\n",
    "\n",
    "The corrected chunk softmaxes can be computed as:\n",
    "$$\n",
    "\\mathrm{corrected\\_chunk\\_1\\_softmax} = e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{global\\_max}}\n",
    "$$\n",
    "\n",
    "Let's add and subtract the term `chunk_max_1` to re-write the above as:\n",
    "$$\n",
    "e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{global\\_max}} = e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{global\\_max} - \\mathrm{chunk\\_max}_1 + \\mathrm{chunk\\_max}_1}\n",
    "$$\n",
    "\n",
    "Moving terms around we get:\n",
    "$$\n",
    "e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{global\\_max}} = e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{chunk\\_max}_1} \\cdot e^{\\mathrm{chunk\\_max}_1 - \\mathrm{global\\_max}}\n",
    "$$\n",
    "\n",
    "Tada! If we need to compute the softmax scores in chunks all we need to do is keep track of all the maximums seen so far, compute the global maximum, and then adjust each chunk softmax by multiplying it with the appropriate correction factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fefef",
   "metadata": {},
   "source": [
    "## Online Softmax Denominator\n",
    "Okay now that we've worked out how to compute the numerator in an online fashion, let's look at the denominator.\n",
    "\n",
    "The denominator of the softmax function is the sum of exponentials of all dot-products. Similar to how we kept track of the minimum value seen so far.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{denominator} &= \\sum_{j=1}^{n} e^{x_j}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we split the dot-products into chunks, we can compute the denominator for each chunk separately and then sum them up:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{denominator\\_chunk}_i &= \\sum_{j \\in \\text{chunk}_i} e^{x_j} \\\\\n",
    "\\text{denominator} &= \\sum_{i} \\text{denominator\\_chunk}_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "But since each chunk doesn't know the global maximum we have:\n",
    "$$\n",
    "\\text{denominator\\_chunk}_i = \\sum_{j \\in \\text{chunk}_i} e^{x_j - \\text{chunk\\_max}_i}\n",
    "$$\n",
    "\n",
    "The correct denominator for each chunk can be computed as:\n",
    "$$\n",
    "\\text{corrected\\_denominator\\_chunk}_i = \\sum_{j \\in \\text{chunk}_i} e^{x_j - \\text{global\\_max}} = \\sum_{j \\in \\text{chunk}_i} e^{x_j - \\text{chunk\\_max}_i} \\cdot e^{\\text{chunk\\_max}_i - \\text{global\\_max}}\n",
    "$$\n",
    "\n",
    "The correction factor is the same for all elements within a chunk, so we can factor it out of the sum:\n",
    "$$\n",
    "\\text{corrected\\_denominator\\_chunk}_i = e^{\\text{chunk\\_max}_i - \\text{global\\_max}} \\cdot \\sum_{j \\in \\text{chunk}_i} e^{x_j - \\text{chunk\\_max}_i}\n",
    "$$\n",
    "\n",
    "The overall denominator can then be computed by summing up the corrected denominators from each chunk:\n",
    "$$\n",
    "\\text{denominator} = \\sum_{i} \\text{corrected\\_denominator\\_chunk}_i = \\sum_{i} e^{\\text{chunk\\_max}_i - \\text{global\\_max}} \\cdot \\sum_{j \\in \\text{chunk}_i} e^{x_j - \\text{chunk\\_max}_i}\n",
    "$$\n",
    "\n",
    "We can store the sum of exponentials for each chunk as we compute them, and then apply the correction factor based on the global maximum when we compute the final denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2352deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 2\n",
    "query_chunks = query.chunk(chunksize, dim=0)\n",
    "key_chunks = keys.chunk(chunksize, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6815dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = []\n",
    "dot_products_chunks = []\n",
    "dot_products_sum = []\n",
    "for i, q in enumerate(query_chunks):\n",
    "    for k in key_chunks:\n",
    "        dot_products_chunks.append(q @ k.T) # [q_S, k_S]\n",
    "        maxs.append(torch.max(dot_products_chunks[-1], dim=-1)[0])\n",
    "        dot_products_chunks[-1] -= maxs[-1]\n",
    "        dot_products_sum.append(torch.exp(dot_products_chunks[-1]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "30c813fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1.6802]), tensor([0.8117])]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4741f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max = torch.max(torch.cat(maxs), dim=-1, keepdim=True)[0]\n",
    "correction_factor = [(local_max - global_max) for local_max, dp in zip(maxs, dot_products_chunks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8417f678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-4.9023, -2.5188,  0.0000, -1.4852]]),\n",
       " tensor([[-0.1187, -0.7713,  0.0000, -0.5620]])]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_products_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "980f2c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.]), tensor([-0.8685])]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e17768f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.3144), tensor(2.9205)]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_products_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8d9c2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = sum([\n",
    "    dot_products_sum[i] * torch.exp(correction_factor[i])\n",
    "    for i in range(len(dot_products_chunks))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bb8e3e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5398)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ddbdb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_softmax = [torch.exp(dp + cf)/denominator for dp, cf in zip(dot_products_chunks, correction_factor)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "cf0279ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0029, 0.0317, 0.3937, 0.0892]]),\n",
       " tensor([[0.1467, 0.0764, 0.1652, 0.0942]])]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "cbb91346",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(torch.cat(online_softmax, dim=-1), sofmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279fd77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
