{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5928549e",
   "metadata": {},
   "source": [
    "# Online Softmax\n",
    "Flash Attention has been responsible for cutting down the run-time of transformer models. Flash Attention can be broken down into two sets of optimizations:\n",
    "1. GPU Aware I/O Optimizations. These optimizations are related to how data is moved between GPU (HBM) memory and on-chip (SRAM) memory. These optimizations are not discussed here.\n",
    "2. Online Softmax. This is an algorithmic optimization that allows us to compute softmax in chunks. Each chunk is sized to fit into SRAM memory of the GPU and each chunk can be processed in parallel across multiple streaming multiprocessors (SMs).\n",
    "\n",
    "In this blog post we will discuss the online softmax algorithm and the simple mathematical tricks that makes it possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42c6bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: C:\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --quiet torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29cbecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1211b10",
   "metadata": {},
   "source": [
    "## Softmax Formula\n",
    "\n",
    "Given a vector of scores $z = [z_1, z_2, \\ldots, z_n]$, the softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Since, this blog focuses on helping one understand the math behind online softmax, we will assume that we just have one query vector and multiple key vectors. Thus, we will focus on computing the softmax of a single row of the attention matrix. The same logic can be extended to multiple query vectors and a batch size > 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79581390",
   "metadata": {},
   "source": [
    "Let's initialize a query vector and a random number of key vectors of random hidden size to compute the attention scores and the softmax scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fc9d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax scores: tensor([[1.8617e-03, 2.4752e-06, 9.8138e-01, 1.4424e-04, 3.1028e-04, 1.6145e-05,\n",
      "         1.4872e-03, 1.4797e-02]])\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 2 ** torch.randint(2, 12, (1, ))\n",
    "num_keys = 2 ** torch.randint(2, 8, (1, ))\n",
    "query = torch.randn(1, hidden_dim)\n",
    "keys = torch.randn(num_keys, hidden_dim)\n",
    "\n",
    "dot_products = torch.matmul(query, keys.T)\n",
    "softmax_scores = nn.functional.softmax(dot_products, dim=-1)\n",
    "print(f\"Softmax scores: {softmax_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeef377",
   "metadata": {},
   "source": [
    "## Softmax Subtract By Max\n",
    "To avoid overflow implementations of softmax often subtract all the dot-products by the maximum value in the set. Mathematically this operation is equivalent to normal softmax because of the following:\n",
    "\n",
    "In the numerator and denominator we can factor out a constant \n",
    "$e^{-c}$, where $c = \\max(dot\\_products)$:\n",
    "\n",
    "Numerator:\n",
    "$$\n",
    "e^{dot\\_product_i - c} = e^{dot\\_product_i} \\cdot e^{-c}\n",
    "$$\n",
    "\n",
    "Denominator:\n",
    "$$\n",
    "\\sum_{j=1}^{n} e^{dot\\_product_j - c} = \\sum_{j=1}^{n} e^{dot\\_product_j} \\cdot e^{-c}\n",
    "$$\n",
    "\n",
    "Thus we have:\n",
    "$$\n",
    "\\text{softmax}(dot\\_product_i - c) = \\frac{e^{dot\\_product_i - c}}{\\sum_{j=1}^{n} e^{dot\\_product_j - c}} = \n",
    "\\frac{e^{dot\\_product_i} \\cdot \\cancel{e^{-c}}}{\\sum_{j=1}^{n} e^{dot\\_product_j} \\cdot \\cancel{e^{-c}}} = \n",
    "\\frac{e^{dot\\_product_i}}{\\sum_{j=1}^{n} e^{dot\\_product_j}} = \\text{softmax}(dot\\_product_i)\n",
    "$$\n",
    "\n",
    "**This shows us that subtracting by the max does not change the output of softmax.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea52615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = torch.max(dot_products, dim=-1, keepdim=True)[0]\n",
    "dot_products -= maxs\n",
    "softmax_scores_post_max = torch.nn.functional.softmax(dot_products, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286f4dc",
   "metadata": {},
   "source": [
    "Let's verify that these two softmax computations are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "167fbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(softmax_scores, softmax_scores_post_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0021c",
   "metadata": {},
   "source": [
    "## Online Softmax\n",
    "Great, its time for us to move on to online softmax. To make things simple we'll break down the computation of online softmax into two parts:\n",
    "\n",
    "1. Computing the numerator of softmax in an online fashion.\n",
    "2. Computing the denominator of softmax in an online fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210e84b",
   "metadata": {},
   "source": [
    "## Online Softmax Numerator\n",
    "\n",
    "Let's assume that we have limited memory and cannot compute all the dot-products at once. Instead we need to process them in the max number of chunks that fit in memory.\n",
    "\n",
    "\n",
    "Hold on a minute, in the numerator of softmax we need to subtract each dot-product by the maximum dot-product across all the keys for a given query. If we only have access to a chunk of dot-products at a time, how can we compute the global maximum?\n",
    "\n",
    "\n",
    "We can compute the maximum in an online fashion by keeping track of the maximum value seen so far as we process each chunk. Let's assume that we have two chunks of dot-products:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{chunk}_1 &= [d_1, d_2, \\ldots, d_k] \\\\\n",
    "\\text{chunk\\_max}_1 &= \\max(\\text{chunk}_1) \\\\\n",
    "\\text{chunk\\_1\\_numerator} &= \\text{softmax}(\\text{chunk}_1 - \\text{chunk\\_max}_1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{chunk}_2 &= [d_{k+1}, d_{k+2}, \\ldots, d_n] \\\\\n",
    "\\text{chunk\\_max}_2 &= \\max(\\text{chunk}_2) \\\\\n",
    "\\text{chunk\\_2\\_numerator} &= \\text{softmax}(\\text{chunk}_2 - \\text{chunk\\_max}_2)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "To compute the overall softmax, **we need to adjust the chunk softmaxes based on the difference between the chunk minimums and the global minimum**. We get to this by a little bit of high-school math:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "s &= s_0 + s_1 \\\\\n",
    "e^{s} &= e^{s_0 + s_1} = e^{s_0} \\cdot e^{s_1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "The max can be re-written as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{global\\_max} = \\max(\\text{chunk\\_max}_1, \\text{chunk\\_max}_2)\n",
    "$$\n",
    "\n",
    "\n",
    "The correct numerator for each chunk is:\n",
    "$$\n",
    "\\mathrm{corrected\\_chunk\\_1\\_numerator} = e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{global\\_max}}\n",
    "$$\n",
    "\n",
    "\n",
    "Let's add and subtract the term `chunk_max_1` to re-write the above as:\n",
    "$$\n",
    "e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{global\\_max}} = e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{global\\_max} \\boldsymbol{- \\mathrm{chunk\\_max}_1 + \\mathrm{chunk\\_max}_1}}\n",
    "$$\n",
    "\n",
    "\n",
    "Moving terms around we get:\n",
    "$$\n",
    "e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{global\\_max}} = e^{\\mathrm{dot\\_products\\_chunk}_1 - \\mathrm{chunk\\_max}_1} \\cdot e^{\\mathrm{chunk\\_max}_1 - \\mathrm{global\\_max}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "correction\\_factor\\_chunk\\_1 = e^{\\mathrm{chunk\\_max}_1 - \\mathrm{global\\_max}}\n",
    "$$\n",
    "\n",
    "\n",
    "Tada! If we need to compute the softmax scores in chunks all we need to do is keep track of all the maximums for each chunk and then compute the global maximum. We'll then use the above formula to adjust each chunk numerator by multiplying it with the appropriate correction factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73fefef",
   "metadata": {},
   "source": [
    "## Online Softmax Denominator\n",
    "Okay now that we've worked out how to compute the numerator in an online fashion, let's look at the denominator.\n",
    "\n",
    "The denominator of the softmax function is the sum of exponentials of all dot-products.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{denominator} &= \\sum_{j=1}^{n} e^{dot\\_product_j - \\text{global\\_max}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If we split the dot-products into chunks, we can compute the denominator for each chunk separately and then sum them up:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{denominator\\_chunk}_i &= \\sum_{j \\in \\text{chunk}_i} e^{dot\\_product_j - \\text{global\\_max}} \\\\\n",
    "\\text{denominator} &= \\sum_{i} \\text{denominator\\_chunk}_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "But since each chunk doesn't know the global maximum we'll actually have:\n",
    "$$\n",
    "\\text{denominator\\_chunk}_i = \\sum_{j \\in \\text{chunk}_i} e^{dot\\_product_j - \\text{chunk\\_max}_i}\n",
    "$$\n",
    "\n",
    "The correct denominator for each chunk can be computed as:\n",
    "$$\n",
    "\\text{corrected\\_denominator\\_chunk}_i = \\sum_{j \\in \\text{chunk}_i} e^{\\text{dot\\_product}_j - \\text{global\\_max}} \\\\\n",
    "$$\n",
    "\n",
    "Using the same trick as we did for the numerator, we can add and subtract the term `chunk_max_i`:\n",
    "$$\n",
    "= \\sum_{j \\in \\text{chunk}_i} e^{\\text{dot\\_product}_j - \\text{chunk\\_max}_i} \\cdot e^{\\text{chunk\\_max}_i - \\text{global\\_max}}\n",
    "$$\n",
    "\n",
    "The correction factor is the same for all elements within a chunk, so we can factor it out of the sum:\n",
    "$$\n",
    "\\text{corrected\\_denominator\\_chunk}_i = e^{\\text{chunk\\_max}_i - \\text{global\\_max}} \\cdot \\sum_{j \\in \\text{chunk}_i} e^{dot\\_product_j - \\text{chunk\\_max}_i}\n",
    "$$\n",
    "\n",
    "The overall denominator can then be computed by summing up the corrected denominators from each chunk:\n",
    "$$\n",
    "\\text{denominator} = \\sum_{i} \\text{corrected\\_denominator\\_chunk}_i = \\sum_{i} e^{\\text{chunk\\_max}_i - \\text{global\\_max}} \\cdot \\sum_{j \\in \\text{chunk}_i} e^{dot\\_product_j - \\text{chunk\\_max}_i}\n",
    "$$\n",
    "\n",
    "We can store the sum of exponentials for each chunk as we compute them, and then apply the correction factor based on the global maximum when we compute the final denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028a0ec",
   "metadata": {},
   "source": [
    "## Code Implementation (Naive Version)\n",
    "Alright it's time to convert all this math into code! Lucky for us all of this can be implemented in under 20 lines of PyTorch code. Let's get to it!\n",
    "\n",
    "As a reminder this implementation is a naive version where we don't worry about multiple queries and batch sizes greater than 1. The purpose of this code is to show how we can easily map all the math we've discussed into code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2352deed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "num_chunks = 2 ** torch.randint(2, min(num_keys, 5), (1, )).item()\n",
    "print(f\"Number of chunks: {num_chunks}\")\n",
    "key_chunks = keys.chunk(num_chunks, dim=0)\n",
    "print(len(key_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6815dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = []\n",
    "dot_products_chunks = []\n",
    "dot_products_sum = []\n",
    "for k in key_chunks:\n",
    "    # compute the dot-product against all keys in the chunk\n",
    "    dot_products_chunks.append(query @ k.T)\n",
    "    # store the max\n",
    "    maxs.append(torch.max(dot_products_chunks[-1], dim=-1)[0])\n",
    "    # subtract the dot-product with the max of the chunk\n",
    "    dot_products_chunks[-1] -= maxs[-1]\n",
    "    # store the sum of dot-product scores for the chunk\n",
    "    dot_products_sum.append(torch.exp(dot_products_chunks[-1]).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7c213",
   "metadata": {},
   "source": [
    "In the code above you see that we:\n",
    "\n",
    "1. For a given chunk we compute the dot-product scores.\n",
    "2. Find the max for the chunk.\n",
    "3. Subtract the dot-products with the max of the chunk.\n",
    "4. Store the sum of all the dot-product scores within the given chunk.\n",
    "\n",
    "1-3 is needed for the numerator and denominator since both of them require us to compute\n",
    "$$\n",
    "e^{dot\\_product_j - \\text{chunk\\_max}_i}\n",
    "$$\n",
    "\n",
    "4 is needed for the denominator to compute the term\n",
    "$$\n",
    "\\sum_{j \\in \\text{chunk}_i} e^{dot\\_product_j - \\text{chunk\\_max}_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12429f0",
   "metadata": {},
   "source": [
    "Now that we've computed the max's for each chunk we can compute the global max and our correction factor.\n",
    "Remember correction factor is:\n",
    "$$\n",
    "\\text{correction\\_factor} = e^{\\mathrm{chunk\\_max}_ j- \\mathrm{global\\_max}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4741f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max = torch.max(torch.cat(maxs), dim=-1, keepdim=True)[0]\n",
    "correction_factor = [(local_max - global_max) for local_max in maxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab75a5e",
   "metadata": {},
   "source": [
    "We have everything we need to compute the denominator for the softmax function now!\n",
    "\n",
    "$$\n",
    "\\text{denominator} = \\sum_{i} \\text{corrected\\_denominator\\_chunk}_i = \\sum_{i} e^{\\text{chunk\\_max}_i - \\text{global\\_max}} \\cdot \\sum_{j \\in \\text{chunk}_i} e^{dot\\_product_j - \\text{chunk\\_max}_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{correction\\_factor} = \\sum_{i} e^{\\text{chunk\\_max}_i - \\text{global\\_max}} \\\\\n",
    "\\text{dot\\_products\\_sum}_i = \\sum_{j \\in \\text{chunk}_i} e^{\\text{dot\\_product}_j - \\text{chunk\\_max}_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d9c2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = sum([\n",
    "    torch.exp(correction_factor[i]) * dot_products_sum[i]\n",
    "    for i in range(len(dot_products_chunks))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369e6f4",
   "metadata": {},
   "source": [
    "We can now finally compute the correct softmax scores!\n",
    "The numerator for each chunk is computed by multiplying the chunk numerator with the correction factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddbdb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_softmax = [torch.exp(dp + cf)/denominator for dp, cf in zip(dot_products_chunks, correction_factor)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209ad26",
   "metadata": {},
   "source": [
    "Let's verify that our online softmax implementation matches the standard softmax implementation from PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbb91346",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(torch.cat(online_softmax, dim=-1), softmax_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af6c181",
   "metadata": {},
   "source": [
    "Wohoo! Our online softmax implementation matches the standard softmax implementation from PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2905cb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In practice, the chunks of queries and keys can be processed in parallel across multiple streaming multiprocessors (SMs) on the GPU, allowing for efficient computation of the attention mechanism even with limited memory. If you want to take a look at what a production ready implementation looks like, check out the [liger-kernels](https://github.com/linkedin/Liger-Kernel/blob/0a62700752e8592c0bf16916d1e4dbf598cee8c1/src/liger_kernel/ops/softmax.py#L116) implementation.\n",
    "\n",
    "If you'd like to understand the gpu related I/O optimizations and understand how online softmax is integrated into a full attention mechanism, check out Aleksa Gordic's great blog post [here](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
