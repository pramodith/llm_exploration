{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4836c46a",
   "metadata": {},
   "source": [
    "# Speculative Decoding Tutorial\n",
    "Speculative decoding is a technique used to speed up the generation of tokens from a generative deep learning model. The main idea is to leverage a smaller model often referred to as the \"draft model\" to generate K draft tokens auto-regressively. The target model i.e. the model that we actually want to use for generation then scores these K tokens in parallel, and we accept or reject them based.\n",
    "\n",
    "Speculative decoding also guarantees that the final output matches the one that would have been obtained by using the target model alone, ensuring correctness while improving speed when we use greedy decoding. When we use sampling based decoding strategies like top-k or nucleus sampling etc. speculative decoding (often referred to as speculative sampling in this context) guarantees to reflect the same probability distribution as the target model. **This means that you're guaranteed to get the same outputs as those produced by sampling the target model alone, regardless of decoding strategy.**\n",
    "\n",
    "This blog/notebook shows how to implement speculative decoding using PyTorch and the Hugging Face Transformers library without using a pre-built function for speculative decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa45ddc",
   "metadata": {},
   "source": [
    "Let's install the necessary libraries and set up the environment for speculative decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131a268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m75 packages\u001b[0m \u001b[2min 3.41s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 370ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 27ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add transformers accelerate hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcc168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pramo\\llm_exploration\\transformer_from_scratch\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a12a03",
   "metadata": {},
   "source": [
    "## Draft and Target Models\n",
    "Let's load the draft and target models. In this blog we will use SmolLM2-360M as our draft model and SmolLM2-1.7B as our target model. One of the key requirements for speculative decoding is that the draft model should:\n",
    "\n",
    "1. Be significantly smaller than the target model to ensure faster token generation.\n",
    "2. Uses the same tokenizer as the target model. This ensures that there's a 1-1 mapping between the tokens generated by the draft model and those scored by the target model.\n",
    "\n",
    "_Note: There have been a few algorithms that reconcile the differences in tokenizers between the draft and target models, but they are out of scope for this blog. For more on this checkout [Universal Assisted Generation: Faster Decoding with Any Assistant Model](https://huggingface.co/blog/universal_assisted_generation)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdcc3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\", device_map=\"auto\")\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", device_map=\"auto\")\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2245f",
   "metadata": {},
   "source": [
    "Let's create a couple of prompts to test speculative decoding. Both the prompts guarantee that a decent number of tokens need to be generated and can also easily be verified for correctness. The knowledge intensive nature of the prompts also ensures that the draft model is likely to make mistakes, allowing us to see how speculative decoding handles rejections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The 50 states of the USA in alphabetical order are: \", \n",
    "    \"The countries of South America in alphabetical order are: \",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ba0a5",
   "metadata": {},
   "source": [
    "In this tutorial we'll use the draft model to generate 8 tokens. After every 8 tokens generated by the draft model, the target model will score them in parallel and accept or reject them based.\n",
    "\n",
    "We'll also just focus on greedy decoding for now i.e. the draft model will always pick the token with the highest probability at each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "target_model.eval()\n",
    "num_draft_tokens = 8\n",
    "greedy_gen = GenerationConfig(num_beams=1, do_sample=False, num_draft_tokens=8)\n",
    "if torch.cuda.is_available():\n",
    "  device_type = \"cuda\"\n",
    "elif platform.system() == \"Darwin\" and getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "  device_type = \"mps\"\n",
    "else:\n",
    "  device_type = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08b4b0",
   "metadata": {},
   "source": [
    "Let's sample a prompt and encode them using draft and target model tokenizers. We'll compare the encoded tokens to ensure that both tokenizers produce the same tokens for the same prompt as a way to verify that both models use the same tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b2443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "prompt = random.sample(prompts, 1)[0]\n",
    "inputs = target_tokenizer.encode(prompt, return_tensors=\"pt\").to(device_type)\n",
    "draft_inputs = draft_tokenizer.encode(prompt, return_tensors=\"pt\").to(device_type)\n",
    "assert torch.equal(inputs, draft_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36914cf6",
   "metadata": {},
   "source": [
    "Let's run the prompt through both the draft and target models to see what the outputs from both models look like. We can observe that both the models have slightly different outputs. Remember that our implementation of speculative decoding, should ensure that the final output matches that of the target model exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a142a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 50 states of the USA in alphabetical order are: \\nAlabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, and Wyoming.<|im_end|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft_output_tokens = draft_model.generate(inputs=inputs, generation_config=GenerationConfig(num_beams=1, do_sample=False, max_new_tokens=256, use_cache=True))[0]\n",
    "draft_tokenizer.decode(draft_output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b4891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 50 states of the USA in alphabetical order are: \\nAlabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming.<|im_end|>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_output_tokens = target_model.generate(inputs=inputs, generation_config=GenerationConfig(num_beams=1, do_sample=False, max_new_tokens=256, use_cache=True))[0]\n",
    "target_tokenizer.decode(target_output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0abe6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not torch.equal(draft_output_tokens,target_output_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ce822",
   "metadata": {},
   "source": [
    "To gauge potential speedups from speculative decoding, let's use `%%timeit` to see how long it takes the draft and target models to produce a response for our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.9 s ± 730 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "target_output_tokens = target_model.generate(inputs=inputs, generation_config=GenerationConfig(num_beams=1, do_sample=False, max_new_tokens=256, use_cache=True))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ece387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.47 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "draft_output_tokens = draft_model.generate(inputs=inputs, generation_config=GenerationConfig(num_beams=1, do_sample=False, max_new_tokens=256, use_cache=True))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa5b47d",
   "metadata": {},
   "source": [
    "## Implementing Speculative Decoding\n",
    "In this tutorial we'll assume a batch size of 1 for simplicity.\n",
    "\n",
    "Here are the steps to the algorithm:\n",
    "\n",
    "1. Generate K draft tokens using the draft model.\n",
    "2. Run a single forward pass of the target model to obtain the probability scores of tokens at each of the K positions.\n",
    "3. Check if the token corresponding to the highest probability score assigned by the target model at position i (1 <= i <= K) matches the draft token at position i.\n",
    "4. Identify the first position j where the draft token does not match the target model's highest probability token. Steps 3 and 4. check if there's a discrepancy between the greedy outputs of the draft and target models.\n",
    "5. After the previous step we know that:\n",
    "    - Draft tokens 1 to j-1 would have been generated by the target model as well, so we can accept these tokens.\n",
    "    - Despite draft token j being wrong, we know what the correct token should've been (i.e. the token with the highest probability score assigned by the target model at position j).\n",
    "   So we can prepare out next input sequence by appending the accepted tokens (1 to j-1) and the correct token at position j to our existing input sequence.\n",
    "6. If all K tokens were accepted, we can simply append all K tokens to our input sequence and also append the next token predicted by the target model at position K+1.\n",
    "7. Repeat the process until the desired sequence length is reached or a stop token is generated.\n",
    "\n",
    "\n",
    "#### Why can't we accept tokens at j+1 from the target model?\n",
    "\n",
    "Let's assume that our prompt is `Jack and Jill` and we generate 3 draft tokens using the draft model: `went down a`.\n",
    "\n",
    "1. Target model tokens with highest prob scores:\n",
    "   - Position 1: `went` (matches draft)\n",
    "   - Position 2: `up` (does not match draft `down`)\n",
    "   - Position 3: `to` (does not match draft `a`)\n",
    "2. We accept the token `up` at position 2 because the prefix `Jack and Jill went` would have been generated by the target model as well and auto-regressive generation for the next token depends only on the prefix.\n",
    "3. However, we cannot accept the token `to` at position 3 because the prefix at this position when the model was used for scoring would have been `Jack and Jill went down` since the input to the target model comes from the draft model. So the input/prefix to the target model at position 3 is different from what it would have been if we had generated tokens auto-regressively using the target model alone.\n",
    "\n",
    "#### No forward pass through the target model gets wasted\n",
    "If no draft tokens were accepeted, we still know what the correct token should be at position 1 after the forward pass. So we can append this token to our input sequence and move forward. We already discussed the case for when j or all K tokens are accepted and how we can always append one token from the target model after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b82d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_decoding(prompt, max_new_tokens, gen_config) -> torch.LongTensor:\n",
    "    do_stop = False\n",
    "    inputs = target_tokenizer.encode(prompt, return_tensors=\"pt\").to(device_type)\n",
    "    original_prompt_len = inputs.shape[1]\n",
    "    draft_output = None\n",
    "    target_output = None\n",
    "    while not do_stop:\n",
    "        prompt_len = inputs.shape[1]\n",
    "        # print(f\"Original prompt length is {prompt_len}\")\n",
    "        draft_output = draft_model.generate(\n",
    "            inputs=inputs,\n",
    "            past_key_values=draft_output.past_key_values.to_tuple() if draft_output else None,\n",
    "            generation_config=gen_config,\n",
    "            return_dict_in_generate=True\n",
    "        )[:, prompt_len: ]\n",
    "        draft_tokens = draft_output.sequences\n",
    "        validation_inputs = torch.cat([inputs, draft_tokens], dim=1)\n",
    "        with torch.no_grad():\n",
    "            target_output = target_model(\n",
    "                validation_inputs,\n",
    "                use_cache=True,\n",
    "                return_dict=True,\n",
    "                past_key_values=target_output.past_key_values.to_tuple() if target_output else None\n",
    "            )\n",
    "            logits = target_output.logits[:, prompt_len-1:]\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            model_predicted_tokens = torch.argmax(probs, dim=-1)\n",
    "\n",
    "            # Get probs of last non-pad token only\n",
    "        # draft_token_probs = torch.gather(\n",
    "        #     probs[:, :-1],\n",
    "        #     dim=-1,\n",
    "        #     index=draft_tokens.view(draft_tokens.shape[1], -1),\n",
    "        # )\n",
    "        if not torch.all(model_predicted_tokens[:, :-1] == draft_tokens):\n",
    "            #print(f\"Generated draft tokens {draft_tokenizer.batch_decode(draft_tokens)}\")\n",
    "            mismatch = torch.argwhere(model_predicted_tokens[:, :-1] != draft_tokens)[0][1]\n",
    "            mismatched_token = model_predicted_tokens[0, mismatch]\n",
    "            matched_draft_tokens = draft_tokens[:, :mismatch]\n",
    "            #print(f\"Draft model predicted: {(draft_tokens[0, mismatch] ,tokenizer.decode([draft_tokens[0, mismatch]], skip_special_tokens=False))}\")\n",
    "            #print(f\"Model predicted : {(mismatched_token, tokenizer.decode([mismatched_token], skip_special_tokens=False))}\")\n",
    "            inputs =  torch.cat([inputs, matched_draft_tokens, mismatched_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        else:\n",
    "            inputs =  torch.cat([inputs, draft_tokens, model_predicted_tokens[0, -1].unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "        if target_tokenizer.eos_token_id in inputs or (max_new_tokens - (inputs.shape[1] - original_prompt_len)) < 0:\n",
    "            do_stop=True\n",
    "        #print(f\"Next inputs are : {tokenizer.batch_decode(inputs)}\")\n",
    "    final_answer = inputs[0, : min(torch.argwhere(inputs[0]==target_tokenizer.eos_token_id)+1, max_new_tokens)]\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "468abb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "speculative_decoding_output_tokens = speculative_decoding(prompt, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19130f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(output_tokens, speculative_decoding_output_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
