{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "131a268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m75 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m55 packages\u001b[0m \u001b[2min 19ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcc168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pramodith/ProgrammingProjects/llm_exploration/transformer_from_scratch/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fdcc3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\", device_map=\"auto\")\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "prompts = [\n",
    "    \"The 50 states of the USA in alphabetical order are: \", \n",
    "    \"The countries of South America in alphabetical order are: \",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b41465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "model.eval()\n",
    "num_draft_tokens = 8\n",
    "greedy_gen = GenerationConfig(num_beams=1, do_sample=False, max_new_tokens=8)\n",
    "if torch.cuda.is_available():\n",
    "  device_type = \"cuda\"\n",
    "elif platform.system() == \"Darwin\" and getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "  device_type = \"mps\"\n",
    "else:\n",
    "  device_type = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b27b2443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "prompt = random.sample(prompts, 1)[0]\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device_type)\n",
    "draft_inputs = draft_tokenizer.encode(prompt, return_tensors=\"pt\").to(device_type)\n",
    "assert torch.equal(inputs, draft_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f37a142a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 50 states of the USA in alphabetical order are: \\nAlabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, and Wyoming.<|im_end|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens = draft_model.generate(inputs=inputs, generation_config=GenerationConfig(num_beams=1, do_sample=False, max_new_tokens=256))[0]\n",
    "draft_tokenizer.decode(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "930b4891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 50 states of the USA in alphabetical order are: \\nAlabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming.<|im_end|>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens = model.generate(inputs=inputs, generation_config=GenerationConfig(num_beams=1, do_sample=False, max_new_tokens=256))[0]\n",
    "tokenizer.decode(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e5784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.9 s ± 730 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output_tokens = model.generate(inputs=inputs, generation_config=GenerationConfig(num_beams=1, do_sample=False, max_new_tokens=256))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7ece387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.47 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "draft_output_tokens = draft_model.generate(inputs=inputs, generation_config=GenerationConfig(num_beams=1, do_sample=False, max_new_tokens=256))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04b82d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_decoding(prompt, max_new_tokens) -> torch.LongTensor:\n",
    "    stop_token = tokenizer.eos_token_id\n",
    "    do_stop = False\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device_type)\n",
    "    original_prompt_len = inputs.shape[1]\n",
    "    while not do_stop:\n",
    "        prompt_len = inputs.shape[1]\n",
    "        # print(f\"Original prompt length is {prompt_len}\")\n",
    "        draft_tokens = draft_model.generate(inputs=inputs, generation_config=greedy_gen)[:, prompt_len: ]\n",
    "        validation_inputs = torch.cat([inputs, draft_tokens], dim=1)\n",
    "        with torch.no_grad():\n",
    "            logits = model(validation_inputs).logits[:, prompt_len-1:]\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            model_predicted_tokens = torch.argmax(probs, dim=-1)\n",
    "            \n",
    "            # Get probs of last non-pad token only\n",
    "        # draft_token_probs = torch.gather(\n",
    "        #     probs[:, :-1],\n",
    "        #     dim=-1,\n",
    "        #     index=draft_tokens.view(draft_tokens.shape[1], -1),\n",
    "        # )\n",
    "        if not torch.all(model_predicted_tokens[:, :-1] == draft_tokens):\n",
    "            #print(f\"Generated draft tokens {draft_tokenizer.batch_decode(draft_tokens)}\")\n",
    "            mismatch = torch.argwhere(model_predicted_tokens[:, :-1] != draft_tokens)[0][1]\n",
    "            mismatched_token = model_predicted_tokens[0, mismatch]\n",
    "            matched_draft_tokens = draft_tokens[:, :mismatch]\n",
    "            #print(f\"Draft model predicted: {(draft_tokens[0, mismatch] ,tokenizer.decode([draft_tokens[0, mismatch]], skip_special_tokens=False))}\")\n",
    "            #print(f\"Model predicted : {(mismatched_token, tokenizer.decode([mismatched_token], skip_special_tokens=False))}\")\n",
    "            inputs =  torch.cat([inputs, matched_draft_tokens, mismatched_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "        else:\n",
    "            inputs =  torch.cat([inputs, draft_tokens, model_predicted_tokens[0, -1].unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "        if tokenizer.eos_token_id in inputs or (max_new_tokens - (inputs.shape[1] - original_prompt_len)) < 0:\n",
    "            do_stop=True\n",
    "        #print(f\"Next inputs are : {tokenizer.batch_decode(inputs)}\")\n",
    "    final_answer = inputs[0, : min(torch.argwhere(inputs[0]==tokenizer.eos_token_id)+1, max_new_tokens)]\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "468abb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "speculative_decoding_output_tokens = speculative_decoding(prompt, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19130f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(output_tokens, speculative_decoding_output_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
